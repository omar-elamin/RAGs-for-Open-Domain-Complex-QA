{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Contriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-24T18:37:59.318118Z",
     "iopub.status.busy": "2025-01-24T18:37:59.31778Z",
     "iopub.status.idle": "2025-01-24T18:44:03.241477Z",
     "shell.execute_reply": "2025-01-24T18:44:03.240734Z",
     "shell.execute_reply.started": "2025-01-24T18:37:59.31809Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from dexter.retriever.dense.Contriever import Contriever\n",
    "from dexter.config.constants import Split\n",
    "from dexter.data.loaders.RetrieverDataset import RetrieverDataset\n",
    "from dexter.utils.metrics.retrieval.RetrievalMetrics import RetrievalMetrics\n",
    "from dexter.data.datastructures.hyperparameters.dpr import DenseHyperParams\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "config_path = \"/kaggle/input/contriever/config.ini\"\n",
    "\n",
    "# Configuration setup\n",
    "config_instance = DenseHyperParams(\n",
    "    query_encoder_path=\"facebook/contriever\",\n",
    "    document_encoder_path=\"facebook/contriever\",\n",
    "    batch_size=32\n",
    ")\n",
    "config = config_instance.get_all_params()\n",
    "\n",
    "# Data loading\n",
    "loader = RetrieverDataset(\"wikimultihopqa\", \"wikimultihopqa-corpus\", config_path, Split.DEV)\n",
    "queries, qrels, corpus = loader.qrels()\n",
    "print(type(corpus))\n",
    "print(\"queries\", len(queries), len(qrels), len(corpus), queries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T18:44:34.457013Z",
     "iopub.status.busy": "2025-01-24T18:44:34.456505Z",
     "iopub.status.idle": "2025-01-24T18:44:42.470343Z",
     "shell.execute_reply": "2025-01-24T18:44:42.469323Z",
     "shell.execute_reply.started": "2025-01-24T18:44:34.456982Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tasb_search = Contriever(config_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T18:45:29.935489Z",
     "iopub.status.busy": "2025-01-24T18:45:29.934973Z",
     "iopub.status.idle": "2025-01-24T18:45:31.547419Z",
     "shell.execute_reply": "2025-01-24T18:45:31.546645Z",
     "shell.execute_reply.started": "2025-01-24T18:45:29.935462Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from dexter.data.datastructures.evidence import Evidence\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.cuda.amp import autocast\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "def compute_document_embeddings_batched(corpus, document_encoder, tokenizer):\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(corpus))):\n",
    "        batch = corpus[i]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}  # Move to GPU\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with autocast():  # Mixed precision\n",
    "                outputs = document_encoder(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state[:, 0, :].cpu())\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "# Extract texts from Evidence objects\n",
    "corpus_texts = [evidence.__dict__['_text'] for evidence in corpus]\n",
    "\n",
    "# Initialize tokenizer and document encoder\n",
    "document_tokenizer = AutoTokenizer.from_pretrained(config_instance.document_encoder_path)\n",
    "document_encoder = AutoModel.from_pretrained(config_instance.document_encoder_path, output_hidden_states=True).to(\"cuda\")\n",
    "document_encoder.gradient_checkpointing_enable()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Compute embeddings\n",
    "document_embeddings = compute_document_embeddings_batched(corpus_texts, document_encoder, document_tokenizer)\n",
    "\n",
    "# Save embeddings\n",
    "torch.save(document_embeddings, \"document_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T18:46:13.852936Z",
     "iopub.status.busy": "2025-01-24T18:46:13.85258Z",
     "iopub.status.idle": "2025-01-24T18:46:22.869431Z",
     "shell.execute_reply": "2025-01-24T18:46:22.868722Z",
     "shell.execute_reply.started": "2025-01-24T18:46:13.852908Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "document_embeddings = torch.load(\"/kaggle/input/contriever/document_embeddings.pt\")\n",
    "# Convert embeddings to NumPy\n",
    "embeddings_np = document_embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up index for documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T18:46:40.662387Z",
     "iopub.status.busy": "2025-01-24T18:46:40.662063Z",
     "iopub.status.idle": "2025-01-24T18:46:42.448481Z",
     "shell.execute_reply": "2025-01-24T18:46:42.447751Z",
     "shell.execute_reply.started": "2025-01-24T18:46:40.662362Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Create a FAISS index\n",
    "faiss_index = faiss.IndexFlatIP(document_embeddings.shape[1])\n",
    "faiss_index.add(embeddings_np)  # Add embeddings to the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Query Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T18:46:45.065651Z",
     "iopub.status.busy": "2025-01-24T18:46:45.065322Z",
     "iopub.status.idle": "2025-01-24T18:46:55.040189Z",
     "shell.execute_reply": "2025-01-24T18:46:55.039002Z",
     "shell.execute_reply.started": "2025-01-24T18:46:45.065624Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize the query encoder\n",
    "query_encoder = AutoModel.from_pretrained(\"facebook/contriever\").to('cuda')\n",
    "query_texts = [q.__dict__['_text'] for q in queries] \n",
    "query_inputs = document_tokenizer(query_texts, return_tensors=\"pt\", padding=True, truncation=True).to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    query_outputs = query_encoder(\n",
    "        input_ids=query_inputs[\"input_ids\"],\n",
    "        attention_mask=query_inputs[\"attention_mask\"]\n",
    "    )\n",
    "\n",
    "    query_embeddings = query_outputs.last_hidden_state[:, 0, :]\n",
    "_, hard_negatives = faiss_index.search(query_embeddings.cpu().numpy(), k=3)\n",
    "query_encoder.save_pretrained(\"trained_query_encoder\")\n",
    "document_tokenizer.save_pretrained(\"trained_query_encoder\")\n",
    "torch.save(query_embeddings.cpu().numpy(), \"query_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T18:47:34.404062Z",
     "iopub.status.busy": "2025-01-24T18:47:34.403738Z",
     "iopub.status.idle": "2025-01-24T18:47:34.432173Z",
     "shell.execute_reply": "2025-01-24T18:47:34.43087Z",
     "shell.execute_reply.started": "2025-01-24T18:47:34.404036Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_data = []\n",
    "\n",
    "# Prepare training data\n",
    "for idx, (query_id, positive_docs) in enumerate(qrels.items()):\n",
    "    # Extract positive document indices\n",
    "    positive_indices = list(map(int, positive_docs.keys()))\n",
    "    \n",
    "    # Get the hard negatives for the current query\n",
    "    negative_indices = hard_negatives[idx]  # Use the index of the query in hard_negatives\n",
    "\n",
    "    # Convert hard negative indices to document texts\n",
    "    negative_texts = [corpus[neg_idx].__dict__['_text'] for neg_idx in negative_indices]\n",
    "\n",
    "    # Add training samples\n",
    "    for positive_idx in positive_indices:\n",
    "        training_data.append({\n",
    "            \"query\": query_texts[idx],\n",
    "            \"positive\": corpus[positive_idx].__dict__['_text'],     # Positive document\n",
    "            \"negatives\": negative_texts           # Hard negatives\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T18:47:42.496523Z",
     "iopub.status.busy": "2025-01-24T18:47:42.496211Z",
     "iopub.status.idle": "2025-01-24T18:47:42.501487Z",
     "shell.execute_reply": "2025-01-24T18:47:42.500471Z",
     "shell.execute_reply.started": "2025-01-24T18:47:42.496497Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def lambda_loss(scores_positive, scores_negative, sigma=1.0):\n",
    "    \"\"\"\n",
    "    Lambda Loss for pairwise ranking tasks.\n",
    "\n",
    "    Args:\n",
    "        scores_positive (torch.Tensor): Tensor of similarity scores for positive pairs (batch_size, 1).\n",
    "        scores_negative (torch.Tensor): Tensor of similarity scores for negative pairs (batch_size, num_negatives).\n",
    "        sigma (float): Smoothing factor (default=1.0).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The computed Lambda Loss.\n",
    "    \"\"\"\n",
    "    # Compute pairwise differences\n",
    "    pairwise_diff = scores_positive.unsqueeze(-1) - scores_negative  # Shape: (batch_size, num_negatives)\n",
    "\n",
    "    # Compute pairwise probabilities\n",
    "    probabilities = torch.sigmoid(sigma * pairwise_diff)  # Shape: (batch_size, num_negatives)\n",
    "\n",
    "    # Lambda Loss\n",
    "    loss = -torch.log(probabilities + 1e-12).mean()  # Add a small constant to avoid log(0)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def contrastive_loss(pos_score, neg_scores, margin=1.0):\n",
    "\n",
    "    # Loss\n",
    "    loss = F.relu(margin + neg_scores - pos_score.unsqueeze(1)).mean()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune query encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T18:47:55.762561Z",
     "iopub.status.busy": "2025-01-24T18:47:55.762245Z",
     "iopub.status.idle": "2025-01-24T18:49:58.797112Z",
     "shell.execute_reply": "2025-01-24T18:49:58.796019Z",
     "shell.execute_reply.started": "2025-01-24T18:47:55.762536Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "subset_size = 1200  # Adjust the size as needed\n",
    "subset_training_data = training_data[:subset_size]\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "batch_size = 1\n",
    "\n",
    "query_encoder_path = \"trained_query_encoder\"\n",
    "query_encoder = AutoModel.from_pretrained(query_encoder_path).to('cuda')\n",
    "# document_tokenizer = AutoTokenizer.from_pretrained(query_encoder_path)\n",
    "\n",
    "dataloader = DataLoader(subset_training_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(query_encoder.parameters(), lr=5e-6)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}\", leave=True)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        # Process negatives in chunks\n",
    "        negatives = [neg for sublist in batch[\"negatives\"] for neg in sublist]\n",
    "        negative_embeddings = []\n",
    "        for i in range(len(negatives)):\n",
    "            inputs = document_tokenizer(negatives[i], return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(\"cuda\")\n",
    "            embedding = query_encoder(**inputs).last_hidden_state[:, 0, :]\n",
    "            negative_embeddings.append(embedding)\n",
    "        negative_embeddings = torch.cat(negative_embeddings, dim=0)\n",
    "\n",
    "        queries_input = document_tokenizer(batch[\"query\"], return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "        positives = document_tokenizer(batch[\"positive\"], return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "        \n",
    "        # Encode queries, positives, and negatives\n",
    "        query_emb = query_encoder(**queries_input).last_hidden_state[:, 0, :]\n",
    "        positive_emb = query_encoder(**positives).last_hidden_state[:, 0, :]\n",
    "\n",
    "        scores_positive = torch.matmul(query_embeddings, positive_emb.T)\n",
    "        scores_negative = torch.matmul(query_embeddings, negative_embeddings.T)\n",
    "        \n",
    "        loss = contrastive_loss(scores_positive, scores_negative)\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        # Update progress bar and epoch loss\n",
    "        progress_bar.set_postfix({\"Loss\": loss.item()})\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1} completed with average loss: {epoch_loss / len(dataloader)}\")\n",
    "\n",
    "query_encoder.save_pretrained(\"fine_tuned_query_encoder\")\n",
    "document_tokenizer.save_pretrained(\"fine_tuned_query_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "# Path to the saved query encoder\n",
    "query_encoder_path = \"/kaggle/input/dataset/fine_tuned_query_encoder\"  # Replace with your directory path\n",
    "\n",
    "# Load the model and tokenizer\n",
    "query_encoder = AutoModel.from_pretrained(query_encoder_path).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(query_encoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T18:50:55.85935Z",
     "iopub.status.busy": "2025-01-24T18:50:55.858948Z",
     "iopub.status.idle": "2025-01-24T18:50:58.139149Z",
     "shell.execute_reply": "2025-01-24T18:50:58.137533Z",
     "shell.execute_reply.started": "2025-01-24T18:50:55.859313Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "query_texts = [q.__dict__['_text'] for q in queries] \n",
    "query_inputs = tokenizer(query_texts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    query_embeddings = query_encoder(**query_inputs).last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "# Save the embeddings for reuse\n",
    "torch.save(query_embeddings, \"query_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from dexter.data.datastructures.evidence import Evidence\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.cuda.amp import autocast\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "def compute_document_embeddings_batched(corpus, document_encoder, tokenizer):\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(corpus))):\n",
    "        batch = corpus[i]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}  # Move to GPU\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with autocast():  # Mixed precision\n",
    "                outputs = document_encoder(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state[:, 0, :].cpu())\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "# Extract texts from Evidence objects\n",
    "corpus_texts = [evidence.__dict__['_text'] for evidence in corpus]\n",
    "\n",
    "# Initialize tokenizer and document encoder\n",
    "document_tokenizer = tokenizer\n",
    "document_encoder = query_encoder\n",
    "document_encoder.gradient_checkpointing_enable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Compute embeddings\n",
    "document_embeddings = compute_document_embeddings_batched(corpus_texts, document_encoder, document_tokenizer)\n",
    "\n",
    "# Save embeddings\n",
    "torch.save(document_embeddings, \"document_embeddings.pt\")\n",
    "faiss_index = faiss.IndexFlatIP(document_embeddings.shape[1])\n",
    "faiss_index.add(document_embeddings)\n",
    "torch.save(document_embeddings, \"document_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Path to the saved FAISS index\n",
    "faiss_index_path = \"/kaggle/input/dataset/document_index.faiss\"\n",
    "\n",
    "# Load the index\n",
    "faiss_index = faiss.read_index(faiss_index_path)\n",
    "_, indices = faiss_index.search(query_embeddings, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run The LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "# Load JSON files\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Prepare input for LLM\n",
    "def prepare_input(query, contexts):\n",
    "    user_prompt = f\"\"\"[Question]: When does monsoon season end in the state the area code 575 is located?\n",
    "[Answer]: The area code 575 is located in New Mexico. Monsoon season in New Mexico typically ends in mid-September. So the\n",
    "[Final Answer]: mid-September.\n",
    "[Question]: What is the current official currency in the country where Ineabelle Diaz is a citizen?\n",
    "[Answer]: Ineabelle Diaz is from Peurto Rico, which is in the United States of America. The current official currency in the United\n",
    "States is the United States dollar. \n",
    "[Final Answer]: United States dollar.\n",
    "[Question]: Where was the person who founded the American Institute of Public Opinion in 1935 born?\n",
    "[Answer]: The person who founded the American Institute of Public Opinion in 1935 is George Gallup. George Gallup was born\n",
    "in Jefferson, Iowa. \n",
    "[Final Answer]: Jefferson.\n",
    "[Question]: What language is used by the director of Tiffany Memorandum?\n",
    "[Answer]: The director of Tiffany Memorandum is Sergio Grieco. Sergio Grieco speaks Italian.\n",
    "[Final Answer]: Italian.\n",
    "[Question]: What is the sports team the person played for who scored the first touchdown in Superbowl 1?\n",
    "[Answer]: The player that scored the first touchdown in Superbowl 1 is Max McGee. Max McGee played for the Green Bay\n",
    "Packers.\n",
    "[Final Answer]: Green Bay Packers.\n",
    "[Question]: The birth country of Jayantha Ketagoda left the British Empire when?\n",
    "[Answer]: The birth country of Jayantha Ketagoda is Sri Lanka. Sri Lanka left the British Empire on February 4, 1948. So the\n",
    "[Final Answer]: February 4, 1948.\\n\\n Follow the above example and Given the evidence, Evidence: {' '.join(contexts)} \\n use the information, think step by step and output the final answer extremely concisely in the form [Final Answer]: for the question, Question:{query}\"\"\"\n",
    "    return user_prompt\n",
    "\n",
    "# Generate response using LLM\n",
    "def generate_answer(input_text, tokenizer, model):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512).to('cuda')\n",
    "    inputs.pop(\"token_type_ids\", None)\n",
    "    try:\n",
    "        outputs = model.generate(**inputs)\n",
    "        # Decode the output tokens to text\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        # Return a default response if generation fails\n",
    "        print(f\"Generation error: {e}\")\n",
    "        return \"not possible\"\n",
    "\n",
    "# F1 Score Calculation\n",
    "def f1_score_metric(prediction, ground_truth):\n",
    "    pred_tokens = set(prediction.lower().split())\n",
    "    gt_tokens = set(ground_truth.lower().split())\n",
    "    \n",
    "    intersection = pred_tokens.intersection(gt_tokens)\n",
    "    precision = len(intersection) / len(pred_tokens) if len(pred_tokens) > 0 else 0\n",
    "    recall = len(intersection) / len(gt_tokens) if len(gt_tokens) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1, precision, recall\n",
    "\n",
    "# BLEU Score Calculation\n",
    "def bleu_score_metric(prediction, ground_truth):\n",
    "    prediction_tokens = prediction.lower().split()\n",
    "    ground_truth_tokens = ground_truth.lower().split()\n",
    "    score = sentence_bleu([ground_truth_tokens], prediction_tokens)\n",
    "    return score\n",
    "\n",
    "# ROUGE Score Calculation\n",
    "def rouge_score_metric(prediction, ground_truth):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(ground_truth, prediction)\n",
    "    return scores\n",
    "\n",
    "# Process dataset and evaluate responses\n",
    "def process_and_evaluate(dataset, tokenizer, model):\n",
    "    results = {}\n",
    "    question_df = {\"questions\":[],\"answers\":[]}\n",
    "    total_em = 0\n",
    "    total_f1 = 0\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_bleu = 0\n",
    "    total_rouge = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}\n",
    "    count = 0\n",
    "\n",
    "    for entry in dataset:\n",
    "        if count == 400:\n",
    "            break\n",
    "        else:\n",
    "            count += 1\n",
    "        query_id = entry[\"_id\"]\n",
    "        query = entry[\"question\"]\n",
    "        ground_truth = entry[\"answer\"]\n",
    "        # Retrieve contexts and prepare input\n",
    "        contexts = retrieved_contexts[count]\n",
    "        # print(contexts)\n",
    "        input_text = prepare_input(query, contexts)\n",
    "        # print(input_text)\n",
    "        \n",
    "        # Generate response\n",
    "        prediction = generate_answer(input_text, tokenizer, model)\n",
    "        \n",
    "        # Calculate Exact Match score\n",
    "        if \"not possible\" in prediction.lower() or \"unknown\" in prediction.lower():\n",
    "            em = 0\n",
    "        elif len(re.split(r'\\[?Final Answer\\]?:', prediction)) > 1:\n",
    "            answer = re.split(r'\\[?Final Answer\\]?:', prediction)[-1]\n",
    "            em = 1 if ground_truth.lower() in prediction.lower() else 0\n",
    "        else:\n",
    "            em = 0\n",
    "\n",
    "        total_em += em\n",
    "        \n",
    "        # Calculate F1 Score\n",
    "        f1,precision, recall = f1_score_metric(re.split(r'\\[?Final Answer\\]?:', prediction)[-1], ground_truth)\n",
    "        total_f1 += f1\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        \n",
    "        # Calculate BLEU Score\n",
    "        bleu = bleu_score_metric(re.split(r'\\[?Final Answer\\]?:', prediction)[-1], ground_truth)\n",
    "        total_bleu += bleu\n",
    "        \n",
    "        # Calculate ROUGE Score\n",
    "        rouge = rouge_score_metric(re.split(r'\\[?Final Answer\\]?:', prediction)[-1], ground_truth)\n",
    "        total_rouge['rouge1'] += rouge['rouge1'].fmeasure\n",
    "        total_rouge['rouge2'] += rouge['rouge2'].fmeasure\n",
    "        total_rouge['rougeL'] += rouge['rougeL'].fmeasure\n",
    "        \n",
    "        # Store results\n",
    "        results[query_id] = {\"prediction\": prediction, \"ground_truth\": ground_truth, \"exact_match\": em}\n",
    "        \n",
    "        if len(re.split(r'\\[?Final Answer\\]?:', prediction)) > 1:\n",
    "            question_df[\"answers\"].append(re.split(r'\\[?Final Answer\\]?:', prediction)[-1])\n",
    "        else:\n",
    "            question_df[\"answers\"].append(prediction)\n",
    "            \n",
    "        question_df[\"questions\"].append(query)\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    accuracy = total_em / 1200\n",
    "    average_f1 = total_f1 / 1200\n",
    "    average_precision = total_precision / 1200\n",
    "    average_recall = total_recall / 1200\n",
    "    average_bleu = total_bleu / 1200\n",
    "    average_rouge1 = total_rouge['rouge1'] / 1200\n",
    "    average_rouge2 = total_rouge['rouge2'] / 1200\n",
    "    average_rougeL = total_rouge['rougeL'] / 1200\n",
    "\n",
    "    final_questions = pd.DataFrame(question_df)\n",
    "\n",
    "    print(f\"Exact Match Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Average F1 Score: {average_f1:.2f}\")\n",
    "    print(f\"Average Precision: {average_precision:.2f}\")\n",
    "    print(f\"Average Recall: {average_recall:.2f}\")\n",
    "    print(f\"Average BLEU Score: {average_bleu:.2f}\")\n",
    "    print(f\"Average ROUGE-1 F-Score: {average_rouge1:.2f}\")\n",
    "    print(f\"Average ROUGE-2 F-Score: {average_rouge2:.2f}\")\n",
    "    print(f\"Average ROUGE-L F-Score: {average_rougeL:.2f}\")\n",
    "    \n",
    "    final_questions.to_csv(\"llama2_wqa_rag_5_few_shot_metrics.tsv\", sep=\"\\t\", index=False)\n",
    "    \n",
    "    return results, accuracy, average_f1, average_precision, average_recall, average_bleu, average_rouge1, average_rouge2, average_rougeL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "auth_token = os.getenv(\"huggingface_token\")\n",
    "\n",
    "if not auth_token:\n",
    "    raise ValueError(\"Authentication token not found. Please set huggingface_token in your .env file.\")\n",
    "\n",
    "login(auth_token)\n",
    "\n",
    "# File paths\n",
    "dataset_file_path = \"/kaggle/input/contriever/dev.json\"\n",
    "document_file_path = \"/kaggle/input/contriever/wiki_musique_corpus.json\"\n",
    "\n",
    "# Load data\n",
    "# contriever_data = relevant\n",
    "dataset = load_json(dataset_file_path)\n",
    "document_data = load_json(document_file_path)\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Initialize LLM and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def map_indices_to_texts(indices, corpus):\n",
    "    return [\n",
    "        [corpus[int(idx)].__dict__[\"_text\"] for idx in doc_indices if int(idx) < len(corpus)]\n",
    "        for doc_indices in indices\n",
    "    ]\n",
    "retrieved_contexts = map_indices_to_texts(indices, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_path = \"retrieved_contexts.json\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(retrieved_contexts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results, accuracy, average_f1,average_precision, average_recall, average_bleu, average_rouge1, average_rouge2, average_rougeL = process_and_evaluate(\n",
    "    dataset, tokenizer, model)\n",
    "\n",
    "# print(f\"Results for ratio {ratio}, N={N}\")\n",
    "# Print evaluation results\n",
    "print(f\"Overall Exact Match Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Average F1 Score: {average_f1:.2f}\")\n",
    "print(f\"Average Precision: {average_precision:.2f}\")\n",
    "print(f\"Average Recall: {average_recall:.2f}\")\n",
    "print(f\"Average BLEU Score: {average_bleu:.2f}\")\n",
    "print(f\"Average ROUGE-1 F-Score: {average_rouge1:.2f}\")\n",
    "print(f\"Average ROUGE-2 F-Score: {average_rouge2:.2f}\")\n",
    "print(f\"Average ROUGE-L F-Score: {average_rougeL:.2f}\")\n",
    "\n",
    "with open(f\"/kaggle/working/accuracy_metrics.txt\", \"w\") as f:\n",
    "    f.write(f\"Overall Exact Match Accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "    f.write(f\"Average F1 Score: {average_f1:.2f}\\n\")\n",
    "    f.write(f\"Average Precision: {average_precision:.2f}\\n\")\n",
    "    f.write(f\"Average Recall: {average_recall:.2f}\\n\")\n",
    "    f.write(f\"Average BLEU Score: {average_bleu:.2f}\\n\")\n",
    "    f.write(f\"Average ROUGE-1 F-Score: {average_rouge1:.2f}\\n\")\n",
    "    f.write(f\"Average ROUGE-2 F-Score: {average_rouge2:.2f}\\n\")\n",
    "    f.write(f\"Average ROUGE-L F-Score: {average_rougeL:.2f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6505289,
     "sourceId": 10569943,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6540416,
     "sourceId": 10578558,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
