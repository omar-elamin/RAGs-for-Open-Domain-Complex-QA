{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Evaluate LLMs on retrieved context documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Extract contexts for each query using an off the shelf retriever (contriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-01T22:40:26.679478Z",
     "iopub.status.busy": "2025-01-01T22:40:26.679219Z",
     "iopub.status.idle": "2025-01-01T22:40:26.694119Z",
     "shell.execute_reply": "2025-01-01T22:40:26.693134Z",
     "shell.execute_reply.started": "2025-01-01T22:40:26.679458Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from dexter.retriever.dense.Contriever import Contriever\n",
    "from dexter.config.constants import Split\n",
    "from dexter.data.loaders.RetrieverDataset import RetrieverDataset\n",
    "from dexter.utils.metrics.retrieval.RetrievalMetrics import RetrievalMetrics\n",
    "from dexter.utils.metrics.SimilarityMatch import CosineSimilarity as CosScore\n",
    "from dexter.data.datastructures.hyperparameters.dpr import DenseHyperParams\n",
    "import json\n",
    "\n",
    "\n",
    "config_instance = DenseHyperParams(query_encoder_path=\"facebook/contriever\",\n",
    "                                 document_encoder_path=\"facebook/contriever\"\n",
    "                                 ,batch_size=32)\n",
    "config = config_instance.get_all_params()\n",
    "\n",
    "loader = RetrieverDataset(\"wikimultihopqa\",\"wikimultihopqa-corpus\",\"config/config.ini\",Split.DEV)\n",
    "queries, qrels, corpus = loader.qrels()\n",
    "print(type(corpus))\n",
    "print(\"queries\",len(queries),len(qrels),len(corpus),queries[0])\n",
    "tasb_search = Contriever(config_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T22:40:26.695538Z",
     "iopub.status.busy": "2025-01-01T22:40:26.695251Z",
     "iopub.status.idle": "2025-01-01T22:40:26.708413Z",
     "shell.execute_reply": "2025-01-01T22:40:26.707855Z",
     "shell.execute_reply.started": "2025-01-01T22:40:26.695518Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "k_values=[0,2,4]\n",
    "\n",
    "similarity_measure = CosScore()\n",
    "for k in k_values:\n",
    "    response = tasb_search.retrieve(corpus,queries,k,similarity_measure)\n",
    "    print(\"indices\",len(response))\n",
    "    metrics = RetrievalMetrics()\n",
    "    print(metrics.evaluate_retrieval(qrels=qrels,results=response))\n",
    "\n",
    "    cur = k + 1\n",
    "\n",
    "    # Save the retrieval results to a json file\n",
    "    with open(f\"retrieval_results/retrieval_results_top_{cur}.json\",\"w\") as f:\n",
    "        json.dump(response,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Feed the retrieval results to the LLM and evaluate the accuracy using Exact Match (and metrics like ROUGE and BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot Prompts Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T22:40:33.161955Z",
     "iopub.status.busy": "2025-01-01T22:40:33.161711Z",
     "iopub.status.idle": "2025-01-01T22:40:33.167508Z",
     "shell.execute_reply": "2025-01-01T22:40:33.166610Z",
     "shell.execute_reply.started": "2025-01-01T22:40:33.161935Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import torch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load JSON files\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Map document IDs to their text\n",
    "def get_document_texts(doc_ids, document_data):\n",
    "    return [document_data[str(doc_id)][\"text\"] for doc_id in doc_ids if str(doc_id) in document_data]\n",
    "\n",
    "# Retrieve top-k contexts\n",
    "def retrieve_contexts(query_id, contriever_data, document_data):\n",
    "    doc_ids = list(contriever_data.get(query_id, {}).keys())\n",
    "    return get_document_texts(doc_ids, document_data)\n",
    "\n",
    "# Prepare input for LLM\n",
    "def prepare_input(query, contexts):\n",
    "    user_prompt = f\"Given the evidence, Evidence: {' '.join(contexts)} \\n use the information, think step by step and output the final answer extremely concisely in the form [Final Answer]: for the question, Question:{query}\"\n",
    "    return user_prompt\n",
    "\n",
    "# Generate response using LLM\n",
    "def generate_answer(input_text, tokenizer, model):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True).to('cuda')\n",
    "    outputs = model.generate(**inputs)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# F1 Score Calculation\n",
    "def f1_score_metric(prediction, ground_truth):\n",
    "    pred_tokens = set(prediction.lower().split())\n",
    "    gt_tokens = set(ground_truth.lower().split())\n",
    "    \n",
    "    intersection = pred_tokens.intersection(gt_tokens)\n",
    "    precision = len(intersection) / len(pred_tokens) if len(pred_tokens) > 0 else 0\n",
    "    recall = len(intersection) / len(gt_tokens) if len(gt_tokens) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1\n",
    "\n",
    "# BLEU Score Calculation\n",
    "def bleu_score_metric(prediction, ground_truth):\n",
    "    prediction_tokens = prediction.lower().split()\n",
    "    ground_truth_tokens = ground_truth.lower().split()\n",
    "    score = sentence_bleu([ground_truth_tokens], prediction_tokens)\n",
    "    return score\n",
    "\n",
    "# ROUGE Score Calculation\n",
    "def rouge_score_metric(prediction, ground_truth):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(ground_truth, prediction)\n",
    "    return scores\n",
    "\n",
    "# Process dataset and evaluate responses\n",
    "def process_and_evaluate(contriever_data, dataset, document_data, tokenizer, model):\n",
    "    results = {}\n",
    "    question_df = {\"questions\":[],\"answers\":[]}\n",
    "    total_em = 0\n",
    "    total_f1 = 0\n",
    "    total_bleu = 0\n",
    "    total_rouge = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}\n",
    "    count = 0\n",
    "\n",
    "    for entry in dataset:\n",
    "        if count == 1200:\n",
    "            break\n",
    "        else:\n",
    "            count += 1\n",
    "        query_id = entry[\"_id\"]\n",
    "        query = entry[\"question\"]\n",
    "        ground_truth = entry[\"answer\"]\n",
    "        \n",
    "        # Retrieve contexts and prepare input\n",
    "        contexts = retrieve_contexts(query_id, contriever_data, document_data)\n",
    "        input_text = prepare_input(query, contexts)\n",
    "        \n",
    "        # Generate response\n",
    "        prediction = generate_answer(input_text, tokenizer, model)\n",
    "        \n",
    "        # Calculate Exact Match score\n",
    "        if \"not possible\" in prediction.lower() or \"unknown\" in prediction.lower():\n",
    "            em = 0\n",
    "        elif len(re.split(r'\\[?Final Answer\\]?:', prediction)) > 1:\n",
    "            answer = re.split(r'\\[?Final Answer\\]?:', prediction)[-1]\n",
    "            em = 1 if ground_truth.lower() in prediction.lower() else 0\n",
    "        else:\n",
    "            em = 0\n",
    "\n",
    "        total_em += em\n",
    "        \n",
    "        # Calculate F1 Score\n",
    "        f1 = f1_score_metric(re.split(r'\\[?Final Answer\\]?:', prediction)[-1], ground_truth)\n",
    "        total_f1 += f1\n",
    "        \n",
    "        # Calculate BLEU Score\n",
    "        bleu = bleu_score_metric(re.split(r'\\[?Final Answer\\]?:', prediction)[-1], ground_truth)\n",
    "        total_bleu += bleu\n",
    "        \n",
    "        # Calculate ROUGE Score\n",
    "        rouge = rouge_score_metric(re.split(r'\\[?Final Answer\\]?:', prediction)[-1], ground_truth)\n",
    "        total_rouge['rouge1'] += rouge['rouge1'].fmeasure\n",
    "        total_rouge['rouge2'] += rouge['rouge2'].fmeasure\n",
    "        total_rouge['rougeL'] += rouge['rougeL'].fmeasure\n",
    "        \n",
    "        # Store results\n",
    "        results[query_id] = {\"prediction\": prediction, \"ground_truth\": ground_truth, \"exact_match\": em}\n",
    "        \n",
    "        if len(re.split(r'\\[?Final Answer\\]?:', prediction)) > 1:\n",
    "            question_df[\"answers\"].append(re.split(r'\\[?Final Answer\\]?:', prediction)[-1])\n",
    "        else:\n",
    "            question_df[\"answers\"].append(prediction)\n",
    "            \n",
    "        question_df[\"questions\"].append(query)\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    accuracy = total_em / 1200\n",
    "    average_f1 = total_f1 / 1200\n",
    "    average_bleu = total_bleu / 1200\n",
    "    average_rouge1 = total_rouge['rouge1'] / 1200\n",
    "    average_rouge2 = total_rouge['rouge2'] / 1200\n",
    "    average_rougeL = total_rouge['rougeL'] / 1200\n",
    "\n",
    "    final_questions = pd.DataFrame(question_df)\n",
    "\n",
    "    print(f\"Exact Match Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Average F1 Score: {average_f1:.2f}\")\n",
    "    print(f\"Average BLEU Score: {average_bleu:.2f}\")\n",
    "    print(f\"Average ROUGE-1 F-Score: {average_rouge1:.2f}\")\n",
    "    print(f\"Average ROUGE-2 F-Score: {average_rouge2:.2f}\")\n",
    "    print(f\"Average ROUGE-L F-Score: {average_rougeL:.2f}\")\n",
    "    \n",
    "    final_questions.to_csv(\"llama2_wqa_rag_3_zero_shot_metrics.tsv\", sep=\"\\t\", index=False)\n",
    "    \n",
    "    return results, accuracy, average_f1, average_bleu, average_rouge1, average_rouge2, average_rougeL\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "auth_token = os.getenv(\"huggingface_token\")\n",
    "\n",
    "if not auth_token:\n",
    "    raise ValueError(\"Authentication token not found. Please set HF_TOKEN in your .env file.\")\n",
    "\n",
    "login(auth_token)\n",
    "\n",
    "# File paths\n",
    "contriever_file_path = \"/kaggle/input/contriever-results-correct/retrieval_results_top_3.json\"\n",
    "dataset_file_path = \"/kaggle/input/dataset/dev.json\"\n",
    "document_file_path = \"/kaggle/input/corpus/wiki_musique_corpus.json\"\n",
    "\n",
    "# Load data\n",
    "contriever_data = load_json(contriever_file_path)\n",
    "dataset = load_json(dataset_file_path)\n",
    "document_data = load_json(document_file_path)\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Initialize LLM and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "# Process dataset and evaluate\n",
    "results, accuracy, average_f1, average_bleu, average_rouge1, average_rouge2, average_rougeL = process_and_evaluate(\n",
    "    contriever_data, dataset, document_data, tokenizer, model)\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Overall Exact Match Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Average F1 Score: {average_f1:.2f}\")\n",
    "print(f\"Average BLEU Score: {average_bleu:.2f}\")\n",
    "print(f\"Average ROUGE-1 F-Score: {average_rouge1:.2f}\")\n",
    "print(f\"Average ROUGE-2 F-Score: {average_rouge2:.2f}\")\n",
    "print(f\"Average ROUGE-L F-Score: {average_rougeL:.2f}\")\n",
    "\n",
    "with open(\"/kaggle/working/accuracy_metrics.txt\", \"w\") as f:\n",
    "    f.write(f\"Overall Exact Match Accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "    f.write(f\"Average F1 Score: {average_f1:.2f}\\n\")\n",
    "    f.write(f\"Average BLEU Score: {average_bleu:.2f}\\n\")\n",
    "    f.write(f\"Average ROUGE-1 F-Score: {average_rouge1:.2f}\\n\")\n",
    "    f.write(f\"Average ROUGE-2 F-Score: {average_rouge2:.2f}\\n\")\n",
    "    f.write(f\"Average ROUGE-L F-Score: {average_rougeL:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-Shot Prompts Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T22:40:33.169037Z",
     "iopub.status.busy": "2025-01-01T22:40:33.168727Z",
     "iopub.status.idle": "2025-01-01T22:40:33.191399Z",
     "shell.execute_reply": "2025-01-01T22:40:33.190474Z",
     "shell.execute_reply.started": "2025-01-01T22:40:33.169004Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import torch\n",
    "\n",
    "# Load JSON files\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Map document IDs to their text\n",
    "def get_document_texts(doc_ids, document_data):\n",
    "    return [document_data[str(doc_id)][\"text\"] for doc_id in doc_ids if str(doc_id) in document_data]\n",
    "\n",
    "# Retrieve top-k contexts\n",
    "def retrieve_contexts(query_id, contriever_data, document_data):\n",
    "    doc_ids = list(contriever_data.get(query_id, {}).keys())\n",
    "    return get_document_texts(doc_ids, document_data)\n",
    "\n",
    "# Prepare input for LLM\n",
    "def prepare_input(query, contexts):\n",
    "    user_prompt = f\"\"\"[Question]: When does monsoon season end in the state the area code 575 is located?\n",
    "[Answer]: The area code 575 is located in New Mexico. Monsoon season in New Mexico typically ends in mid-September. So the\n",
    "[Final Answer]: mid-September.\n",
    "[Question]: What is the current official currency in the country where Ineabelle Diaz is a citizen?\n",
    "[Answer]: Ineabelle Diaz is from Peurto Rico, which is in the United States of America. The current official currency in the United\n",
    "States is the United States dollar. \n",
    "[Final Answer]: United States dollar.\n",
    "[Question]: Where was the person who founded the American Institute of Public Opinion in 1935 born?\n",
    "[Answer]: The person who founded the American Institute of Public Opinion in 1935 is George Gallup. George Gallup was born\n",
    "in Jefferson, Iowa. \n",
    "[Final Answer]: Jefferson.\n",
    "[Question]: What language is used by the director of Tiffany Memorandum?\n",
    "[Answer]: The director of Tiffany Memorandum is Sergio Grieco. Sergio Grieco speaks Italian.\n",
    "[Final Answer]: Italian.\n",
    "[Question]: What is the sports team the person played for who scored the first touchdown in Superbowl 1?\n",
    "[Answer]: The player that scored the first touchdown in Superbowl 1 is Max McGee. Max McGee played for the Green Bay\n",
    "Packers.\n",
    "[Final Answer]: Green Bay Packers.\n",
    "[Question]: The birth country of Jayantha Ketagoda left the British Empire when?\n",
    "[Answer]: The birth country of Jayantha Ketagoda is Sri Lanka. Sri Lanka left the British Empire on February 4, 1948. So the\n",
    "[Final Answer]: February 4, 1948.\\n\\n Follow the above example and Given the evidence, Evidence: {' '.join(contexts)} \\n use the information, think step by step and output the final answer extremely concisely in the form [Final Answer]: for the question, Question:{query}\"\"\"\n",
    "    return user_prompt\n",
    "\n",
    "# Generate response using LLM\n",
    "def generate_answer(input_text, tokenizer, model):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True).to('cuda')\n",
    "    outputs = model.generate(**inputs)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# F1 Score Calculation\n",
    "def f1_score_metric(prediction, ground_truth):\n",
    "    pred_tokens = set(prediction.lower().split())\n",
    "    gt_tokens = set(ground_truth.lower().split())\n",
    "    \n",
    "    intersection = pred_tokens.intersection(gt_tokens)\n",
    "    precision = len(intersection) / len(pred_tokens) if len(pred_tokens) > 0 else 0\n",
    "    recall = len(intersection) / len(gt_tokens) if len(gt_tokens) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1\n",
    "\n",
    "# BLEU Score Calculation\n",
    "def bleu_score_metric(prediction, ground_truth):\n",
    "    prediction_tokens = prediction.lower().split()\n",
    "    ground_truth_tokens = ground_truth.lower().split()\n",
    "    score = sentence_bleu([ground_truth_tokens], prediction_tokens)\n",
    "    return score\n",
    "\n",
    "# ROUGE Score Calculation\n",
    "def rouge_score_metric(prediction, ground_truth):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(ground_truth, prediction)\n",
    "    return scores\n",
    "\n",
    "# Process dataset and evaluate responses\n",
    "def process_and_evaluate(contriever_data, dataset, document_data, tokenizer, model):\n",
    "    results = {}\n",
    "    question_df = {\"questions\":[],\"answers\":[]}\n",
    "    total_em = 0\n",
    "    total_f1 = 0\n",
    "    total_bleu = 0\n",
    "    total_rouge = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}\n",
    "    count = 0\n",
    "\n",
    "    for entry in dataset:\n",
    "        if count == 1200:\n",
    "            break\n",
    "        else:\n",
    "            count += 1\n",
    "        query_id = entry[\"_id\"]\n",
    "        query = entry[\"question\"]\n",
    "        ground_truth = entry[\"answer\"]\n",
    "        \n",
    "        # Retrieve contexts and prepare input\n",
    "        contexts = retrieve_contexts(query_id, contriever_data, document_data)\n",
    "        input_text = prepare_input(query, contexts)\n",
    "        \n",
    "        # Generate response\n",
    "        prediction = generate_answer(input_text, tokenizer, model)\n",
    "        \n",
    "        # Calculate Exact Match score\n",
    "        if \"not possible\" in prediction.lower() or \"unknown\" in prediction.lower():\n",
    "            em = 0\n",
    "        elif len(re.split(r'\\[?Final Answer\\]?:', prediction)) > 1:\n",
    "            answer = re.split(r'\\[?Final Answer\\]?:', prediction)[-1]\n",
    "            em = 1 if ground_truth.lower() in prediction.lower() else 0\n",
    "        else:\n",
    "            em = 0\n",
    "\n",
    "        total_em += em\n",
    "        \n",
    "        # Calculate F1 Score\n",
    "        f1 = f1_score_metric(re.split(r'\\[?Final Answer\\]?:', prediction)[-1], ground_truth)\n",
    "        total_f1 += f1\n",
    "        \n",
    "        # Calculate BLEU Score\n",
    "        bleu = bleu_score_metric(re.split(r'\\[?Final Answer\\]?:', prediction)[-1], ground_truth)\n",
    "        total_bleu += bleu\n",
    "        \n",
    "        # Calculate ROUGE Score\n",
    "        rouge = rouge_score_metric(re.split(r'\\[?Final Answer\\]?:', prediction)[-1], ground_truth)\n",
    "        total_rouge['rouge1'] += rouge['rouge1'].fmeasure\n",
    "        total_rouge['rouge2'] += rouge['rouge2'].fmeasure\n",
    "        total_rouge['rougeL'] += rouge['rougeL'].fmeasure\n",
    "        \n",
    "        # Store results\n",
    "        results[query_id] = {\"prediction\": prediction, \"ground_truth\": ground_truth, \"exact_match\": em}\n",
    "        \n",
    "        if len(re.split(r'\\[?Final Answer\\]?:', prediction)) > 1:\n",
    "            question_df[\"answers\"].append(re.split(r'\\[?Final Answer\\]?:', prediction)[-1])\n",
    "        else:\n",
    "            question_df[\"answers\"].append(prediction)\n",
    "            \n",
    "        question_df[\"questions\"].append(query)\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    accuracy = total_em / 1200\n",
    "    average_f1 = total_f1 / 1200\n",
    "    average_bleu = total_bleu / 1200\n",
    "    average_rouge1 = total_rouge['rouge1'] / 1200\n",
    "    average_rouge2 = total_rouge['rouge2'] / 1200\n",
    "    average_rougeL = total_rouge['rougeL'] / 1200\n",
    "\n",
    "    final_questions = pd.DataFrame(question_df)\n",
    "\n",
    "    print(f\"Exact Match Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Average F1 Score: {average_f1:.2f}\")\n",
    "    print(f\"Average BLEU Score: {average_bleu:.2f}\")\n",
    "    print(f\"Average ROUGE-1 F-Score: {average_rouge1:.2f}\")\n",
    "    print(f\"Average ROUGE-2 F-Score: {average_rouge2:.2f}\")\n",
    "    print(f\"Average ROUGE-L F-Score: {average_rougeL:.2f}\")\n",
    "    \n",
    "    final_questions.to_csv(\"llama2_wqa_rag_5_few_shot_metrics.tsv\", sep=\"\\t\", index=False)\n",
    "    \n",
    "    return results, accuracy, average_f1, average_bleu, average_rouge1, average_rouge2, average_rougeL\n",
    "\n",
    "auth_token = os.getenv(\"huggingface_token\")\n",
    "\n",
    "if not auth_token:\n",
    "    raise ValueError(\"Authentication token not found. Please set HF_TOKEN in your .env file.\")\n",
    "\n",
    "login(auth_token)\n",
    "\n",
    "# File paths\n",
    "contriever_file_path = \"/kaggle/input/contriever-results-correct/retrieval_results_top_5.json\"\n",
    "dataset_file_path = \"/kaggle/input/dataset/dev.json\"\n",
    "document_file_path = \"/kaggle/input/corpus/wiki_musique_corpus.json\"\n",
    "\n",
    "# Load data\n",
    "contriever_data = load_json(contriever_file_path)\n",
    "dataset = load_json(dataset_file_path)\n",
    "document_data = load_json(document_file_path)\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Initialize LLM and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "# Process dataset and evaluate\n",
    "results, accuracy, average_f1, average_bleu, average_rouge1, average_rouge2, average_rougeL = process_and_evaluate(\n",
    "    contriever_data, dataset, document_data, tokenizer, model)\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Overall Exact Match Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Average F1 Score: {average_f1:.2f}\")\n",
    "print(f\"Average BLEU Score: {average_bleu:.2f}\")\n",
    "print(f\"Average ROUGE-1 F-Score: {average_rouge1:.2f}\")\n",
    "print(f\"Average ROUGE-2 F-Score: {average_rouge2:.2f}\")\n",
    "print(f\"Average ROUGE-L F-Score: {average_rougeL:.2f}\")\n",
    "\n",
    "with open(\"/kaggle/working/accuracy_metrics.txt\", \"w\") as f:\n",
    "    f.write(f\"Overall Exact Match Accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "    f.write(f\"Average F1 Score: {average_f1:.2f}\\n\")\n",
    "    f.write(f\"Average BLEU Score: {average_bleu:.2f}\\n\")\n",
    "    f.write(f\"Average ROUGE-1 F-Score: {average_rouge1:.2f}\\n\")\n",
    "    f.write(f\"Average ROUGE-2 F-Score: {average_rouge2:.2f}\\n\")\n",
    "    f.write(f\"Average ROUGE-L F-Score: {average_rougeL:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Repeat the above experiment without the retriever, using only oracle contexts as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot Prompts Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-01T22:45:15.328Z",
     "iopub.execute_input": "2025-01-01T22:40:36.800215Z",
     "iopub.status.busy": "2025-01-01T22:40:36.799849Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import torch\n",
    "\n",
    "# Load JSON files\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Retrieve top-k most similar evidence\n",
    "def get_top_k_similar_instances(sentence, data_emb, data, k, threshold, model):\n",
    "    sent_emb = model.encode(sentence)\n",
    "    similarities = cosine_similarity(data_emb, [sent_emb]).flatten()\n",
    "    sorted_indices = similarities.argsort()[::-1]\n",
    "    top_evidences = []\n",
    "    \n",
    "    for idx in sorted_indices[:k]:\n",
    "        if similarities[idx] > threshold:\n",
    "            top_evidences.append(data[idx])\n",
    "    return top_evidences\n",
    "\n",
    "# Prepare input for LLM\n",
    "def prepare_input(query, contexts):\n",
    "    user_prompt = f\"Given the evidence, Evidence: {'; '.join(contexts)} \\n use the information, think step by step and output the final answer extremely concisely in the form [Final Answer]: for the question, Question:{query}\"\n",
    "    return user_prompt\n",
    "\n",
    "# Generate response using LLM\n",
    "def generate_answer(input_text, tokenizer, model):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True).to('cuda')\n",
    "    outputs = model.generate(**inputs)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# F1 Score Calculation\n",
    "def f1_score_metric(prediction, ground_truth):\n",
    "    pred_tokens = set(prediction.lower().split())\n",
    "    gt_tokens = set(ground_truth.lower().split())\n",
    "    \n",
    "    intersection = pred_tokens.intersection(gt_tokens)\n",
    "    precision = len(intersection) / len(pred_tokens) if len(pred_tokens) > 0 else 0\n",
    "    recall = len(intersection) / len(gt_tokens) if len(gt_tokens) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1\n",
    "\n",
    "# BLEU Score Calculation\n",
    "def bleu_score_metric(prediction, ground_truth):\n",
    "    prediction_tokens = prediction.lower().split()\n",
    "    ground_truth_tokens = ground_truth.lower().split()\n",
    "    score = sentence_bleu([ground_truth_tokens], prediction_tokens)\n",
    "    return score\n",
    "\n",
    "# ROUGE Score Calculation\n",
    "def rouge_score_metric(prediction, ground_truth):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(ground_truth, prediction)\n",
    "    return scores\n",
    "\n",
    "# Process dataset and evaluate responses\n",
    "def process_and_evaluate(dataset, tokenizer, model, sentence_model, k=3, threshold=0.5):\n",
    "    results = {}\n",
    "    question_df = {\"questions\":[],\"answers\":[]}\n",
    "    total_em = 0\n",
    "    total_f1 = 0\n",
    "    total_bleu = 0\n",
    "    total_rouge = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}\n",
    "    count = 0\n",
    "\n",
    "    # Precompute embeddings for all evidence\n",
    "    all_evidences = [item[\"evidences\"] for item in dataset]\n",
    "    evidence_texts = [', '.join([' - '.join(map(str, sublist)) for sublist in ev]) for ev in all_evidences]\n",
    "    evidence_emb = sentence_model.encode(evidence_texts)\n",
    "\n",
    "    for entry in dataset:\n",
    "        if count == 1200:\n",
    "            break\n",
    "        else:\n",
    "            count += 1\n",
    "        query_id = entry[\"_id\"]\n",
    "        query = entry[\"question\"]\n",
    "        ground_truth = entry[\"answer\"]\n",
    "        \n",
    "       # Retrieve top-k contexts\n",
    "        top_evidences = get_top_k_similar_instances(query, evidence_emb, evidence_texts, k, threshold, sentence_model)\n",
    "        \n",
    "        input_text = prepare_input(query, top_evidences)\n",
    "        \n",
    "        # Generate response\n",
    "        prediction = generate_answer(input_text, tokenizer, model)\n",
    "        print(prediction)\n",
    "        \n",
    "        # Calculate Exact Match score\n",
    "        if \"not possible\" in prediction.lower() or \"unknown\" in prediction.lower():\n",
    "            em = 0\n",
    "        elif len(re.split(r'\\[?Final Answer\\]?:', prediction)) > 1:\n",
    "            answer = re.split(r'\\[?Final Answer\\]?:', prediction)[-1]\n",
    "            em = 1 if ground_truth.lower() in prediction.lower() else 0\n",
    "        else:\n",
    "            em = 0\n",
    "\n",
    "        total_em += em\n",
    "        \n",
    "        # Calculate F1 Score\n",
    "        f1 = f1_score_metric(re.split(r'\\[?Final Answer\\]?:', prediction)[-1], ground_truth)\n",
    "        total_f1 += f1\n",
    "        \n",
    "        # Calculate BLEU Score\n",
    "        bleu = bleu_score_metric(re.split(r'\\[?Final Answer\\]?:', prediction)[-1], ground_truth)\n",
    "        total_bleu += bleu\n",
    "        \n",
    "        # Calculate ROUGE Score\n",
    "        rouge = rouge_score_metric(re.split(r'\\[?Final Answer\\]?:', prediction)[-1], ground_truth)\n",
    "        total_rouge['rouge1'] += rouge['rouge1'].fmeasure\n",
    "        total_rouge['rouge2'] += rouge['rouge2'].fmeasure\n",
    "        total_rouge['rougeL'] += rouge['rougeL'].fmeasure\n",
    "        \n",
    "        # Store results\n",
    "        results[query_id] = {\"prediction\": prediction, \"ground_truth\": ground_truth, \"exact_match\": em}\n",
    "        \n",
    "        if len(re.split(r'\\[?Final Answer\\]?:', prediction)) > 1:\n",
    "            question_df[\"answers\"].append(re.split(r'\\[?Final Answer\\]?:', prediction)[-1])\n",
    "        else:\n",
    "            question_df[\"answers\"].append(prediction)\n",
    "            \n",
    "        question_df[\"questions\"].append(query)\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    accuracy = total_em / 1200\n",
    "    average_f1 = total_f1 / 1200\n",
    "    average_bleu = total_bleu / 1200\n",
    "    average_rouge1 = total_rouge['rouge1'] / 1200\n",
    "    average_rouge2 = total_rouge['rouge2'] / 1200\n",
    "    average_rougeL = total_rouge['rougeL'] / 1200\n",
    "\n",
    "    final_questions = pd.DataFrame(question_df)\n",
    "\n",
    "    print(f\"Exact Match Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Average F1 Score: {average_f1:.2f}\")\n",
    "    print(f\"Average BLEU Score: {average_bleu:.2f}\")\n",
    "    print(f\"Average ROUGE-1 F-Score: {average_rouge1:.2f}\")\n",
    "    print(f\"Average ROUGE-2 F-Score: {average_rouge2:.2f}\")\n",
    "    print(f\"Average ROUGE-L F-Score: {average_rougeL:.2f}\")\n",
    "    \n",
    "    final_questions.to_csv(\"llama2_wqa_rag_oracle_zero_shot_top_5_metrics.tsv\", sep=\"\\t\", index=False)\n",
    "    \n",
    "    return results, accuracy, average_f1, average_bleu, average_rouge1, average_rouge2, average_rougeL\n",
    "\n",
    "auth_token = os.getenv(\"huggingface_token\")\n",
    "\n",
    "if not auth_token:\n",
    "    raise ValueError(\"Authentication token not found. Please set HF_TOKEN in your .env file.\")\n",
    "\n",
    "login(auth_token)\n",
    "\n",
    "# File paths\n",
    "dataset_file_path = \"/kaggle/input/dataset/dev.json\"\n",
    "\n",
    "# Load data\n",
    "dataset = load_json(dataset_file_path)\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Initialize LLM and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "sentence_model = SentenceTransformer(\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "# Evaluate with top-k retrieval\n",
    "results, accuracy, average_f1, average_bleu, average_rouge1, average_rouge2, average_rougeL = process_and_evaluate(\n",
    "    dataset, tokenizer, model, sentence_model, k=5, threshold=0.5)\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Overall Exact Match Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Average F1 Score: {average_f1:.2f}\")\n",
    "print(f\"Average BLEU Score: {average_bleu:.2f}\")\n",
    "print(f\"Average ROUGE-1 F-Score: {average_rouge1:.2f}\")\n",
    "print(f\"Average ROUGE-2 F-Score: {average_rouge2:.2f}\")\n",
    "print(f\"Average ROUGE-L F-Score: {average_rougeL:.2f}\")\n",
    "\n",
    "with open(\"/kaggle/working/accuracy_metrics.txt\", \"w\") as f:\n",
    "    f.write(f\"Overall Exact Match Accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "    f.write(f\"Average F1 Score: {average_f1:.2f}\\n\")\n",
    "    f.write(f\"Average BLEU Score: {average_bleu:.2f}\\n\")\n",
    "    f.write(f\"Average ROUGE-1 F-Score: {average_rouge1:.2f}\\n\")\n",
    "    f.write(f\"Average ROUGE-2 F-Score: {average_rouge2:.2f}\\n\")\n",
    "    f.write(f\"Average ROUGE-L F-Score: {average_rougeL:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-Shot Prompts Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-01T22:45:15.329Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Load JSON files\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Retrieve top-k most similar evidence\n",
    "def get_top_k_similar_instances(sentence, data_emb, data, k, threshold, model):\n",
    "    sent_emb = model.encode(sentence)\n",
    "    similarities = cosine_similarity(data_emb, [sent_emb]).flatten()\n",
    "    sorted_indices = similarities.argsort()[::-1]\n",
    "    top_evidences = []\n",
    "    \n",
    "    for idx in sorted_indices[:k]:\n",
    "        if similarities[idx] > threshold:\n",
    "            top_evidences.append(data[idx])\n",
    "    return top_evidences\n",
    "\n",
    "# Prepare input for LLM\n",
    "def prepare_input(query, contexts):\n",
    "    user_prompt = f\"\"\"[Question]: When does monsoon season end in the state the area code 575 is located?\n",
    "[Answer]: The area code 575 is located in New Mexico. Monsoon season in New Mexico typically ends in mid-September. So the\n",
    "[Final Answer]: mid-September.\n",
    "[Question]: What is the current official currency in the country where Ineabelle Diaz is a citizen?\n",
    "[Answer]: Ineabelle Diaz is from Peurto Rico, which is in the United States of America. The current official currency in the United\n",
    "States is the United States dollar. \n",
    "[Final Answer]: United States dollar.\n",
    "[Question]: Where was the person who founded the American Institute of Public Opinion in 1935 born?\n",
    "[Answer]: The person who founded the American Institute of Public Opinion in 1935 is George Gallup. George Gallup was born\n",
    "in Jefferson, Iowa. \n",
    "[Final Answer]: Jefferson.\n",
    "[Question]: What language is used by the director of Tiffany Memorandum?\n",
    "[Answer]: The director of Tiffany Memorandum is Sergio Grieco. Sergio Grieco speaks Italian.\n",
    "[Final Answer]: Italian.\n",
    "[Question]: What is the sports team the person played for who scored the first touchdown in Superbowl 1?\n",
    "[Answer]: The player that scored the first touchdown in Superbowl 1 is Max McGee. Max McGee played for the Green Bay\n",
    "Packers.\n",
    "[Final Answer]: Green Bay Packers.\n",
    "[Question]: The birth country of Jayantha Ketagoda left the British Empire when?\n",
    "[Answer]: The birth country of Jayantha Ketagoda is Sri Lanka. Sri Lanka left the British Empire on February 4, 1948. So the\n",
    "[Final Answer]: February 4, 1948.\\n\\n Given the evidence, Evidence: {'; '.join(contexts)} \\n use the information, think step by step and output the final answer extremely concisely in the form [Final Answer]: for the question, Question:{query}\"\"\"\n",
    "    return user_prompt\n",
    "\n",
    "# Generate response using LLM\n",
    "def generate_answer(input_text, tokenizer, model):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True).to('cuda')\n",
    "    outputs = model.generate(**inputs)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# F1 Score Calculation\n",
    "def f1_score_metric(prediction, ground_truth):\n",
    "    pred_tokens = set(prediction.lower().split())\n",
    "    gt_tokens = set(ground_truth.lower().split())\n",
    "    \n",
    "    intersection = pred_tokens.intersection(gt_tokens)\n",
    "    precision = len(intersection) / len(pred_tokens) if len(pred_tokens) > 0 else 0\n",
    "    recall = len(intersection) / len(gt_tokens) if len(gt_tokens) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1\n",
    "\n",
    "# BLEU Score Calculation\n",
    "def bleu_score_metric(prediction, ground_truth):\n",
    "    prediction_tokens = prediction.lower().split()\n",
    "    ground_truth_tokens = ground_truth.lower().split()\n",
    "    score = sentence_bleu([ground_truth_tokens], prediction_tokens)\n",
    "    return score\n",
    "\n",
    "# ROUGE Score Calculation\n",
    "def rouge_score_metric(prediction, ground_truth):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(ground_truth, prediction)\n",
    "    return scores\n",
    "\n",
    "# Process dataset and evaluate responses\n",
    "def process_and_evaluate(dataset, tokenizer, model, sentence_model, k=3, threshold=0.5):\n",
    "    results = {}\n",
    "    question_df = {\"questions\":[],\"answers\":[]}\n",
    "    total_em = 0\n",
    "    total_f1 = 0\n",
    "    total_bleu = 0\n",
    "    total_rouge = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}\n",
    "    count = 0\n",
    "\n",
    "    # Precompute embeddings for all evidence\n",
    "    all_evidences = [item[\"evidences\"] for item in dataset]\n",
    "    evidence_texts = [', '.join([' - '.join(map(str, sublist)) for sublist in ev]) for ev in all_evidences]\n",
    "    evidence_emb = sentence_model.encode(evidence_texts)\n",
    "\n",
    "    for entry in dataset:\n",
    "        if count == 1200:\n",
    "            break\n",
    "        else:\n",
    "            count += 1\n",
    "        query_id = entry[\"_id\"]\n",
    "        query = entry[\"question\"]\n",
    "        ground_truth = entry[\"answer\"]\n",
    "        \n",
    "       # Retrieve top-k contexts\n",
    "        top_evidences = get_top_k_similar_instances(query, evidence_emb, evidence_texts, k, threshold, sentence_model)\n",
    "        \n",
    "        input_text = prepare_input(query, top_evidences)\n",
    "        \n",
    "        # Generate response\n",
    "        prediction = generate_answer(input_text, tokenizer, model)\n",
    "\n",
    "        \n",
    "        # Calculate Exact Match score\n",
    "        if \"not possible\" in prediction.lower() or \"unknown\" in prediction.lower():\n",
    "            em = 0\n",
    "        elif len(re.split(r'\\[?Final Answer\\]?:', prediction)) > 1:\n",
    "            answer = re.split(r'\\[?Final Answer\\]?:', prediction)[-1]\n",
    "            em = 1 if ground_truth.lower() in prediction.lower() else 0\n",
    "        else:\n",
    "            em = 0\n",
    "\n",
    "        total_em += em\n",
    "        \n",
    "        # Calculate F1 Score\n",
    "        f1 = f1_score_metric(re.split(r'\\[?Final Answer\\]?:', prediction)[-1], ground_truth)\n",
    "        total_f1 += f1\n",
    "        \n",
    "        # Calculate BLEU Score\n",
    "        bleu = bleu_score_metric(re.split(r'\\[?Final Answer\\]?:', prediction)[-1], ground_truth)\n",
    "        total_bleu += bleu\n",
    "        \n",
    "        # Calculate ROUGE Score\n",
    "        rouge = rouge_score_metric(re.split(r'\\[?Final Answer\\]?:', prediction)[-1], ground_truth)\n",
    "        total_rouge['rouge1'] += rouge['rouge1'].fmeasure\n",
    "        total_rouge['rouge2'] += rouge['rouge2'].fmeasure\n",
    "        total_rouge['rougeL'] += rouge['rougeL'].fmeasure\n",
    "        \n",
    "        # Store results\n",
    "        results[query_id] = {\"prediction\": prediction, \"ground_truth\": ground_truth, \"exact_match\": em}\n",
    "        \n",
    "        if len(re.split(r'\\[?Final Answer\\]?:', prediction)) > 1:\n",
    "            question_df[\"answers\"].append(re.split(r'\\[?Final Answer\\]?:', prediction)[-1])\n",
    "        else:\n",
    "            question_df[\"answers\"].append(prediction)\n",
    "            \n",
    "        question_df[\"questions\"].append(query)\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    accuracy = total_em / 1200\n",
    "    average_f1 = total_f1 / 1200\n",
    "    average_bleu = total_bleu / 1200\n",
    "    average_rouge1 = total_rouge['rouge1'] / 1200\n",
    "    average_rouge2 = total_rouge['rouge2'] / 1200\n",
    "    average_rougeL = total_rouge['rougeL'] / 1200\n",
    "\n",
    "    final_questions = pd.DataFrame(question_df)\n",
    "\n",
    "    print(f\"Exact Match Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Average F1 Score: {average_f1:.2f}\")\n",
    "    print(f\"Average BLEU Score: {average_bleu:.2f}\")\n",
    "    print(f\"Average ROUGE-1 F-Score: {average_rouge1:.2f}\")\n",
    "    print(f\"Average ROUGE-2 F-Score: {average_rouge2:.2f}\")\n",
    "    print(f\"Average ROUGE-L F-Score: {average_rougeL:.2f}\")\n",
    "    \n",
    "    final_questions.to_csv(\"llama2_wqa_rag_oracle_few_shot_top_3_metrics.tsv\", sep=\"\\t\", index=False)\n",
    "    \n",
    "    return results, accuracy, average_f1, average_bleu, average_rouge1, average_rouge2, average_rougeL\n",
    "\n",
    "auth_token = os.getenv(\"huggingface_token\")\n",
    "\n",
    "if not auth_token:\n",
    "    raise ValueError(\"Authentication token not found. Please set HF_TOKEN in your .env file.\")\n",
    "\n",
    "login(auth_token)\n",
    "\n",
    "# File paths\n",
    "dataset_file_path = \"/kaggle/input/dataset/dev.json\"\n",
    "\n",
    "# Load data\n",
    "dataset = load_json(dataset_file_path)\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Initialize LLM and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "sentence_model = SentenceTransformer(\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "# Evaluate with top-k retrieval\n",
    "results, accuracy, average_f1, average_bleu, average_rouge1, average_rouge2, average_rougeL = process_and_evaluate(\n",
    "    dataset, tokenizer, model, sentence_model, k=3, threshold=0.5)\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Overall Exact Match Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Average F1 Score: {average_f1:.2f}\")\n",
    "print(f\"Average BLEU Score: {average_bleu:.2f}\")\n",
    "print(f\"Average ROUGE-1 F-Score: {average_rouge1:.2f}\")\n",
    "print(f\"Average ROUGE-2 F-Score: {average_rouge2:.2f}\")\n",
    "print(f\"Average ROUGE-L F-Score: {average_rougeL:.2f}\")\n",
    "\n",
    "with open(\"/kaggle/working/accuracy_metrics.txt\", \"w\") as f:\n",
    "    f.write(f\"Overall Exact Match Accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "    f.write(f\"Average F1 Score: {average_f1:.2f}\\n\")\n",
    "    f.write(f\"Average BLEU Score: {average_bleu:.2f}\\n\")\n",
    "    f.write(f\"Average ROUGE-1 F-Score: {average_rouge1:.2f}\\n\")\n",
    "    f.write(f\"Average ROUGE-2 F-Score: {average_rouge2:.2f}\\n\")\n",
    "    f.write(f\"Average ROUGE-L F-Score: {average_rougeL:.2f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6336369,
     "sourceId": 10245429,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6336405,
     "sourceId": 10245476,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6336354,
     "sourceId": 10245499,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6401353,
     "sourceId": 10337803,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
