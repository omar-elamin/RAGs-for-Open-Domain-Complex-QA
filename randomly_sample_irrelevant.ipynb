{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T08:09:43.612463Z",
     "iopub.status.busy": "2025-01-23T08:09:43.612285Z",
     "iopub.status.idle": "2025-01-23T08:11:29.562300Z",
     "shell.execute_reply": "2025-01-23T08:11:29.561666Z",
     "shell.execute_reply.started": "2025-01-23T08:09:43.612444Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "Token has not been saved to git credential helper.\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358ef0b804cc4c5897b61d0b294ccfaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f3ba6ea497466581ed2dd98bf2573e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43f9a617a314353becf5e9193a3fea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817375def8ee4e74add25ca5a99f18d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adaad0572a064c1e9c3a554d4e42c672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca4130435dc4250a49f97111b34df9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260f163e98624d81b92bce0fba5c9ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a69aea384b4a2495f87ed8c46c76bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346e6026b19a477799f71217c5e8c0cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f24760282df7470c96c111f0c7f3963c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e968c8c355094849b27c471ea8f19a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import json\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "auth_token = os.getenv(\"huggingface_token\")\n",
    "\n",
    "if not auth_token:\n",
    "    raise ValueError(\"Authentication token not found. Please set HF_TOKEN in your .env file.\")\n",
    "\n",
    "login(auth_token, add_to_git_credential=True)\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Initialize LLM and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T08:11:53.262239Z",
     "iopub.status.busy": "2025-01-23T08:11:53.261958Z",
     "iopub.status.idle": "2025-01-23T08:17:25.669011Z",
     "shell.execute_reply": "2025-01-23T08:17:25.668107Z",
     "shell.execute_reply.started": "2025-01-23T08:11:53.262216Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dexter-cqa==1.0.9\n",
      "  Downloading dexter_cqa-1.0.9-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting sentence-transformers (from dexter-cqa==1.0.9)\n",
      "  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pytrec-eval (from dexter-cqa==1.0.9)\n",
      "  Downloading pytrec_eval-0.5.tar.gz (15 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting faiss-cpu (from dexter-cqa==1.0.9)\n",
      "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Collecting elasticsearch==7.9.1 (from dexter-cqa==1.0.9)\n",
      "  Downloading elasticsearch-7.9.1-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting data (from dexter-cqa==1.0.9)\n",
      "  Downloading data-0.4.tar.gz (7.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from dexter-cqa==1.0.9) (0.10.2)\n",
      "Collecting zope.interface (from dexter-cqa==1.0.9)\n",
      "  Downloading zope.interface-7.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers==4.30.0 (from dexter-cqa==1.0.9)\n",
      "  Downloading transformers-4.30.0-py3-none-any.whl.metadata (113 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from dexter-cqa==1.0.9) (3.20.3)\n",
      "Collecting openai (from dexter-cqa==1.0.9)\n",
      "  Downloading openai-1.60.0-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: annoy in /usr/local/lib/python3.10/dist-packages (from dexter-cqa==1.0.9) (1.17.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from dexter-cqa==1.0.9) (1.4.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dexter-cqa==1.0.9) (4.66.5)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from dexter-cqa==1.0.9) (2.1.4)\n",
      "Requirement already satisfied: ujson in /usr/local/lib/python3.10/dist-packages (from dexter-cqa==1.0.9) (5.10.0)\n",
      "Requirement already satisfied: gitpython in /usr/local/lib/python3.10/dist-packages (from dexter-cqa==1.0.9) (3.1.43)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from elasticsearch==7.9.1->dexter-cqa==1.0.9) (2.2.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from elasticsearch==7.9.1->dexter-cqa==1.0.9) (2024.8.30)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0->dexter-cqa==1.0.9) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0->dexter-cqa==1.0.9) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0->dexter-cqa==1.0.9) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0->dexter-cqa==1.0.9) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0->dexter-cqa==1.0.9) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0->dexter-cqa==1.0.9) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0->dexter-cqa==1.0.9) (2.32.3)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.0->dexter-cqa==1.0.9)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0->dexter-cqa==1.0.9) (0.4.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from data->dexter-cqa==1.0.9) (1.16.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from data->dexter-cqa==1.0.9) (4.4.2)\n",
      "Collecting funcsigs (from data->dexter-cqa==1.0.9)\n",
      "  Downloading funcsigs-1.0.2-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython->dexter-cqa==1.0.9) (4.0.11)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai->dexter-cqa==1.0.9) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai->dexter-cqa==1.0.9) (1.7.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai->dexter-cqa==1.0.9)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai->dexter-cqa==1.0.9)\n",
      "  Downloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai->dexter-cqa==1.0.9) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai->dexter-cqa==1.0.9) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai->dexter-cqa==1.0.9) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->dexter-cqa==1.0.9) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->dexter-cqa==1.0.9) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->dexter-cqa==1.0.9) (2024.1)\n",
      "INFO: pip is looking at multiple versions of sentence-transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting sentence-transformers (from dexter-cqa==1.0.9)\n",
      "  Downloading sentence_transformers-3.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading sentence_transformers-3.2.0-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading sentence_transformers-3.1.0-py3-none-any.whl.metadata (23 kB)\n",
      "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading sentence_transformers-3.0.0-py3-none-any.whl.metadata (10 kB)\n",
      "INFO: pip is still looking at multiple versions of sentence-transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading sentence_transformers-2.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading sentence_transformers-2.6.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading sentence_transformers-2.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading sentence_transformers-2.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading sentence_transformers-2.4.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading sentence_transformers-2.3.1-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading sentence_transformers-2.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->dexter-cqa==1.0.9) (2.4.1+cu121)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->dexter-cqa==1.0.9) (0.19.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->dexter-cqa==1.0.9) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->dexter-cqa==1.0.9) (1.13.1)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->dexter-cqa==1.0.9) (3.2.4)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->dexter-cqa==1.0.9) (0.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.interface->dexter-cqa==1.0.9) (71.0.4)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->dexter-cqa==1.0.9) (3.10)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->dexter-cqa==1.0.9) (1.2.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython->dexter-cqa==1.0.9) (5.0.1)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai->dexter-cqa==1.0.9)\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai->dexter-cqa==1.0.9)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0->dexter-cqa==1.0.9) (2024.6.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai->dexter-cqa==1.0.9) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai->dexter-cqa==1.0.9) (2.23.4)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers->dexter-cqa==1.0.9) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers->dexter-cqa==1.0.9) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers->dexter-cqa==1.0.9) (3.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.0->dexter-cqa==1.0.9) (3.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers->dexter-cqa==1.0.9) (3.5.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers->dexter-cqa==1.0.9) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers->dexter-cqa==1.0.9) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers->dexter-cqa==1.0.9) (1.3.0)\n",
      "Downloading dexter_cqa-1.0.9-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.0/294.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading elasticsearch-7.9.1-py2.py3-none-any.whl (219 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.2/219.2 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.30.0-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.60.0-py3-none-any.whl (456 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.1/456.1 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading zope.interface-7.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (254 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.5/254.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading funcsigs-1.0.2-py2.py3-none-any.whl (17 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: data, pytrec-eval, sentence-transformers\n",
      "  Building wheel for data (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for data: filename=data-0.4-py3-none-any.whl size=7228 sha256=0c4ff0306f600452be7e7912de2203ba92a71c318d2fea432ad7f530ee55f849\n",
      "  Stored in directory: /root/.cache/pip/wheels/d0/e8/fa/e253c256048ea58d99a8abb5e751abb6a838af6f12887b5418\n",
      "  Building wheel for pytrec-eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pytrec-eval: filename=pytrec_eval-0.5-cp310-cp310-linux_x86_64.whl size=308214 sha256=906940733763af70096523356e620690010e5f88fe53ee0be5d04b11a66080e9\n",
      "  Stored in directory: /root/.cache/pip/wheels/51/3a/cd/dcc1ddfc763987d5cb237165d8ac249aa98a23ab90f67317a8\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125924 sha256=56bb87b533826f7736507e2d71d3c1c3fbb2a5c63c5d95a03eb2ef4c0de89e83\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
      "Successfully built data pytrec-eval sentence-transformers\n",
      "Installing collected packages: tokenizers, funcsigs, zope.interface, pytrec-eval, jiter, h11, faiss-cpu, elasticsearch, data, httpcore, transformers, httpx, sentence-transformers, openai, dexter-cqa\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.44.2\n",
      "    Uninstalling transformers-4.44.2:\n",
      "      Successfully uninstalled transformers-4.44.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kaggle-environments 1.16.10 requires transformers>=4.33.1, but you have transformers 4.30.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed data-0.4 dexter-cqa-1.0.9 elasticsearch-7.9.1 faiss-cpu-1.9.0.post1 funcsigs-1.0.2 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 jiter-0.8.2 openai-1.60.0 pytrec-eval-0.5 sentence-transformers-2.2.2 tokenizers-0.13.3 transformers-4.30.0 zope.interface-7.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412a6046766e42608622614b364f4998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3fd4533237c4560b458cf17a7dbb82c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b2176db7aa4299bb965c7e157b71a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3205550dec9d4c679a901b83b49d9a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  \n",
      "Loading passages: 100%|██████████| 563424/563424 [00:00<00:00, 754460.20it/s]\n",
      "Transforming passage dataset: 100%|██████████| 563424/563424 [00:01<00:00, 319028.73it/s]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harley-Davidson Harley-Davidson\n",
      "KeysView(<Section: Data-Path>)\n",
      "12576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [02:58<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded of length 12000\n",
      "<class 'list'>\n",
      "queries 1200 1200 563424 <dexter.data.datastructures.question.Question object at 0x7bda78dcfa60>\n"
     ]
    }
   ],
   "source": [
    "from dexter.retriever.dense.Contriever import Contriever\n",
    "from dexter.config.constants import Split\n",
    "from dexter.data.loaders.RetrieverDataset import RetrieverDataset\n",
    "from dexter.utils.metrics.retrieval.RetrievalMetrics import RetrievalMetrics\n",
    "from dexter.utils.metrics.SimilarityMatch import CosineSimilarity as CosScore\n",
    "from dexter.data.datastructures.hyperparameters.dpr import DenseHyperParams\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "config_instance = DenseHyperParams(query_encoder_path=\"facebook/contriever\",\n",
    "                                 document_encoder_path=\"facebook/contriever\"\n",
    "                                 ,batch_size=32)\n",
    "config = config_instance.get_all_params()\n",
    "\n",
    "loader = RetrieverDataset(\"wikimultihopqa\",\"wikimultihopqa-corpus\",\"/kaggle/input/config/config.ini\",Split.DEV)\n",
    "queries, qrels, corpus = loader.qrels()\n",
    "\n",
    "print(type(corpus))\n",
    "print(\"queries\",len(queries),len(qrels),len(corpus),queries[0])\n",
    "# tasb_search = Contriever(config_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T08:17:25.670570Z",
     "iopub.status.busy": "2025-01-23T08:17:25.670087Z",
     "iopub.status.idle": "2025-01-23T08:17:29.659443Z",
     "shell.execute_reply": "2025-01-23T08:17:29.658580Z",
     "shell.execute_reply.started": "2025-01-23T08:17:25.670546Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Load the dev.json file\n",
    "with open('/kaggle/input/nlp-assignment/dev.json') as f:\n",
    "    dev = json.load(f)\n",
    "\n",
    "# Load the corpus\n",
    "with open('/kaggle/input/nlp-assignment/wiki_musique_corpus.json') as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "# Load top5 relevant\n",
    "with open('/kaggle/input/retrieval-results/retrieval_results_top_5.json') as f:\n",
    "    top5 = json.load(f)\n",
    "\n",
    "# Extract relevant documents per query\n",
    "def extract_relevant_documents_per_query(query_id, topk):\n",
    "    doc_ids = list(topk.get(query_id, {}).keys())\n",
    "    relevant_docs = [corpus[str(doc_id)][\"text\"]\n",
    "                for doc_id in doc_ids\n",
    "                if str(doc_id) in corpus]\n",
    "    return relevant_docs\n",
    "\n",
    "# Prepare input for LLM\n",
    "def prepare_input(query, contexts):\n",
    "    user_prompt = f\"Given the evidence, Evidence: {' '.join(contexts)} \\n use the information, think step by step and output the final answer extremely concisely in the form [Final Answer]: for the question, Question:{query}\"\n",
    "    return user_prompt\n",
    "\n",
    "# Generate response using LLM\n",
    "def generate_answer(input_text, tokenizer, model):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=3584).to('cuda')\n",
    "    outputs = model.generate(**inputs, max_new_tokens=512)\n",
    "    temp = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return temp\n",
    "\n",
    "def get_document_texts(doc_ids, document_data):\n",
    "    return [document_data[str(doc_id)][\"text\"] for doc_id in doc_ids if str(doc_id) in document_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T08:17:37.838233Z",
     "iopub.status.busy": "2025-01-23T08:17:37.837937Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.2.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=f166fe6f8afed9484c6132d4bc500980a933dbb5ac9739b4292e378359483ce6\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 queries...\n",
      "Processed 20 queries...\n",
      "Processed 30 queries...\n",
      "Processed 40 queries...\n",
      "Processed 50 queries...\n",
      "Processed 60 queries...\n",
      "Processed 70 queries...\n",
      "Processed 80 queries...\n",
      "Processed 90 queries...\n",
      "Processed 100 queries...\n",
      "Average exact_match for 0 out of 5: 0.2500\n",
      "Average f1_score for 0 out of 5: 0.0885\n",
      "Average bleu_score for 0 out of 5: 0.1117\n",
      "Average rouge1 for 0 out of 5: 0.1149\n",
      "Average rouge2 for 0 out of 5: 0.0834\n",
      "Average rougeL for 0 out of 5: 0.1149\n",
      "Average exact_match for 1 out of 5: 0.2600\n",
      "Average f1_score for 1 out of 5: 0.0955\n",
      "Average bleu_score for 1 out of 5: 0.1299\n",
      "Average rouge1 for 1 out of 5: 0.1207\n",
      "Average rouge2 for 1 out of 5: 0.0735\n",
      "Average rougeL for 1 out of 5: 0.1207\n",
      "Average exact_match for 2 out of 5: 0.2600\n",
      "Average f1_score for 2 out of 5: 0.1297\n",
      "Average bleu_score for 2 out of 5: 0.1761\n",
      "Average rouge1 for 2 out of 5: 0.1625\n",
      "Average rouge2 for 2 out of 5: 0.1149\n",
      "Average rougeL for 2 out of 5: 0.1625\n",
      "Average exact_match for 3 out of 5: 0.2100\n",
      "Average f1_score for 3 out of 5: 0.0844\n",
      "Average bleu_score for 3 out of 5: 0.1108\n",
      "Average rouge1 for 3 out of 5: 0.0955\n",
      "Average rouge2 for 3 out of 5: 0.0601\n",
      "Average rougeL for 3 out of 5: 0.0955\n",
      "Average exact_match for 4 out of 5: 0.2200\n",
      "Average f1_score for 4 out of 5: 0.0949\n",
      "Average bleu_score for 4 out of 5: 0.1325\n",
      "Average rouge1 for 4 out of 5: 0.1227\n",
      "Average rouge2 for 4 out of 5: 0.0974\n",
      "Average rougeL for 4 out of 5: 0.1227\n",
      "Average exact_match for 5 out of 5: 0.2000\n",
      "Average f1_score for 5 out of 5: 0.0839\n",
      "Average bleu_score for 5 out of 5: 0.1265\n",
      "Average rouge1 for 5 out of 5: 0.1133\n",
      "Average rouge2 for 5 out of 5: 0.0778\n",
      "Average rougeL for 5 out of 5: 0.1108\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import torch\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# F1 Score Calculation\n",
    "def f1_score_metric(prediction, ground_truth):\n",
    "    pred_tokens = set(prediction.lower().split())\n",
    "    gt_tokens = set(ground_truth.lower().split())\n",
    "    \n",
    "    intersection = pred_tokens.intersection(gt_tokens)\n",
    "    precision = len(intersection) / len(pred_tokens) if len(pred_tokens) > 0 else 0\n",
    "    recall = len(intersection) / len(gt_tokens) if len(gt_tokens) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1\n",
    "\n",
    "# BLEU Score Calculation\n",
    "def bleu_score_metric(prediction, ground_truth):\n",
    "    prediction_tokens = prediction.lower().split()\n",
    "    ground_truth_tokens = ground_truth.lower().split()\n",
    "    score = sentence_bleu([ground_truth_tokens], prediction_tokens)\n",
    "    return score\n",
    "\n",
    "# ROUGE Score Calculation\n",
    "def rouge_score_metric(prediction, ground_truth):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(ground_truth, prediction)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# Maximum retries before skipping the sample\n",
    "MAX_RETRIES = 2\n",
    "\n",
    "def evaluate_noise_ratio(dev, corpus, tokenizer, model, N, num_samples):\n",
    "    results = {}\n",
    "    \n",
    "    for i, entry in enumerate(dev[600:num_samples]):\n",
    "        # print(f\"Processing query {i + 1}...\")\n",
    "        query_id = entry['_id']\n",
    "        query = entry['question']\n",
    "        ground_truth = entry['answer']\n",
    "        # ids = list(qrels[queries[i]._idx].keys())\n",
    "\n",
    "        relevant = extract_relevant_documents_per_query(query_id, top5)\n",
    "        # golden = get_document_texts(ids, corpus)\n",
    "        \n",
    "        # Get combined documents\n",
    "        for j in range(N+1):\n",
    "            # print(f\"Processing noise ratio {j} out of {N}...\")\n",
    "            noise_k = N - j\n",
    "        \n",
    "            all_docs = [doc[\"text\"] for doc in corpus.values()]\n",
    "            irrelevant_docs = [doc for doc in all_docs if doc not in relevant]\n",
    "        \n",
    "            random.seed(42)\n",
    "            sampled_irrelevant = random.sample(irrelevant_docs, k=noise_k)\n",
    "        \n",
    "            combined_docs = relevant[:j] + sampled_irrelevant\n",
    "            \n",
    "            # Prepare input and generate response\n",
    "            input_text = prepare_input(query, combined_docs)\n",
    "            \n",
    "            retries = 0\n",
    "            success = False\n",
    "            while retries < MAX_RETRIES:\n",
    "                try:\n",
    "                    prediction = generate_answer(input_text, tokenizer, model)\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    if \"not possible\" in prediction.lower() or \"unknown\" in prediction.lower():\n",
    "                        em = 0\n",
    "                        prediction_text = prediction\n",
    "                    else:\n",
    "                        answer_parts = re.split(r'\\[?Final Answer\\]?:', prediction)\n",
    "                        prediction_text = answer_parts[-1].strip() if len(answer_parts) > 1 else prediction\n",
    "                        em = 1 if ground_truth.lower() in prediction_text.lower() else 0\n",
    "    \n",
    "                    \n",
    "                    # Update metrics for current noise ratio\n",
    "\n",
    "                    f1 = f1_score_metric(prediction_text, ground_truth)\n",
    "                    bleu = bleu_score_metric(prediction_text, ground_truth)\n",
    "                    rouge = rouge_score_metric(prediction_text, ground_truth)\n",
    "\n",
    "                    if f\"{j} out of {N}\" not in results:\n",
    "                        results[f\"{j} out of {N}\"] = {\n",
    "                            'exact_match': [],\n",
    "                            'f1_score': [],\n",
    "                            'bleu_score': [],\n",
    "                            'rouge1': [],\n",
    "                            'rouge2': [],\n",
    "                            'rougeL': []\n",
    "                        }\n",
    "                    \n",
    "                    results[f\"{j} out of {N}\"]['exact_match'].append(em)\n",
    "                    results[f\"{j} out of {N}\"]['f1_score'].append(f1)\n",
    "                    results[f\"{j} out of {N}\"]['bleu_score'].append(bleu)\n",
    "                    results[f\"{j} out of {N}\"]['rouge1'].append(rouge['rouge1'].fmeasure)\n",
    "                    results[f\"{j} out of {N}\"]['rouge2'].append(rouge['rouge2'].fmeasure)\n",
    "                    results[f\"{j} out of {N}\"]['rougeL'].append(rouge['rougeL'].fmeasure)\n",
    "                    \n",
    "                    success = True\n",
    "                    break\n",
    "                \n",
    "                except RuntimeError as e:\n",
    "                    print(e)\n",
    "                    print(f\"CUDA error encountered. Retrying... ({retries + 1}/{MAX_RETRIES})\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    retries += 1\n",
    "    \n",
    "            if not success:\n",
    "                print(f\"Failed after {MAX_RETRIES} retries. Skipping sample.\")                \n",
    "            \n",
    "            # Free GPU memory after processing each sample\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i + 1} queries...\")\n",
    "        if (i + 1) % 100 == 0:\n",
    "            for k in results.keys():\n",
    "                # Write results to file\n",
    "                with open(f\"/kaggle/working/accuracy_metrics_{i+1}_first_samples_{k}.txt\", \"w\") as f:\n",
    "                    for metric in results[k].keys():\n",
    "                        f.write(f\"Average {metric} for {k}: {np.mean(results[k][metric]):.4f}\\n\")\n",
    "                for metric in results[k].keys():\n",
    "                    print(f\"Average {metric} for {k}: {np.mean(results[k][metric]):.4f}\")\n",
    "    \n",
    "    # Print final result averages \n",
    "    for k in results.keys():\n",
    "        # Write results to file\n",
    "        with open(f\"/kaggle/working/accuracy_metrics_{k}.txt\", \"w\") as f:\n",
    "            for metric in results[k].keys():\n",
    "                f.write(f\"Average {metric} for {k}: {np.mean(results[k][metric]):.4f}\\n\")\n",
    "        for metric in results[k].keys():\n",
    "            print(f\"Average {metric} for {k}: {np.mean(results[k][metric]):.4f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "results = evaluate_noise_ratio(\n",
    "    dev=dev,\n",
    "    corpus=corpus,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    N=5,\n",
    "    num_samples=1200\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6393716,
     "sourceId": 10326221,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6394698,
     "sourceId": 10327698,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6526653,
     "sourceId": 10548477,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
