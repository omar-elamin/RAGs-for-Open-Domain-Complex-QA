{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7Gaf213Npwr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%pip install dexter-cqa==1.0.9\n",
        "%pip install transformers==4.33.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSXAX3xONpwu"
      },
      "source": [
        "# Task 1: Evaluate LLMs on retrieved context documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn8oDK7RNpww"
      },
      "source": [
        "## 1.1 Extract contexts for each query using an off the shelf retriever (contriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab": {
          "referenced_widgets": [
            "2c894427d1164d4dbd232e942040571a",
            "238b8a5c7d9a4a9b8c4fd86f23f915ec",
            "d6c5fcdcd91942e8953e7921dcdb99dd",
            "35f803539f43487a889ad5796e88494b"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-01-23T19:42:19.437397Z",
          "iopub.status.busy": "2025-01-23T19:42:19.437001Z",
          "iopub.status.idle": "2025-01-23T19:47:32.400900Z",
          "shell.execute_reply": "2025-01-23T19:47:32.400128Z",
          "shell.execute_reply.started": "2025-01-23T19:42:19.437356Z"
        },
        "id": "1mOMX1YSNpww",
        "outputId": "3f561213-a09b-4ad7-d489-18b20ffc22f1",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c894427d1164d4dbd232e942040571a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "238b8a5c7d9a4a9b8c4fd86f23f915ec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6c5fcdcd91942e8953e7921dcdb99dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35f803539f43487a889ad5796e88494b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading passages: 100%|██████████| 563424/563424 [00:00<00:00, 654795.00it/s]\n",
            "Transforming passage dataset: 100%|██████████| 563424/563424 [00:01<00:00, 295864.09it/s]\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Harley-Davidson Harley-Davidson\n",
            "KeysView(<Section: Data-Path>)\n",
            "12576\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1200/1200 [03:05<00:00,  6.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded of length 12000\n"
          ]
        }
      ],
      "source": [
        "from dexter.retriever.dense.Contriever import Contriever\n",
        "from dexter.config.constants import Split\n",
        "from dexter.data.loaders.RetrieverDataset import RetrieverDataset\n",
        "from dexter.utils.metrics.retrieval.RetrievalMetrics import RetrievalMetrics\n",
        "from dexter.utils.metrics.SimilarityMatch import CosineSimilarity as CosScore\n",
        "from dexter.data.datastructures.hyperparameters.dpr import DenseHyperParams\n",
        "import json\n",
        "\n",
        "\n",
        "config_instance = DenseHyperParams(query_encoder_path=\"facebook/contriever\",\n",
        "                                 document_encoder_path=\"facebook/contriever\"\n",
        "                                 ,batch_size=32)\n",
        "config = config_instance.get_all_params()\n",
        "\n",
        "loader = RetrieverDataset(\"wikimultihopqa\",\"wikimultihopqa-corpus\",\"/kaggle/input/2wikimultihop/config.ini\",Split.DEV)\n",
        "queries, qrels, corpus = loader.qrels()\n",
        "\n",
        "# print(type(corpus))\n",
        "# print(\"queries\",len(queries),len(qrels),len(corpus),queries[0])\n",
        "# tasb_search = Contriever(config_instance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-23T19:47:32.402571Z",
          "iopub.status.busy": "2025-01-23T19:47:32.402205Z",
          "iopub.status.idle": "2025-01-23T19:47:32.405533Z",
          "shell.execute_reply": "2025-01-23T19:47:32.404875Z",
          "shell.execute_reply.started": "2025-01-23T19:47:32.402550Z"
        },
        "id": "zqn8wUzSNpwy",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "hard_negatives = defaultdict(dict)\n",
        "positives = defaultdict(dict)\n",
        "# filter docs to extract hard negatives\n",
        "for query_id, relevant in docs['retrieved'].items():\n",
        "  positive = qrels[query_id]\n",
        "  for r, score in relevant.items():\n",
        "      if r not in positive:\n",
        "          hard_negatives[query_id][r] = score\n",
        "      else:\n",
        "          positives[query_id][r] = score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-23T19:47:32.406931Z",
          "iopub.status.busy": "2025-01-23T19:47:32.406698Z",
          "iopub.status.idle": "2025-01-23T19:47:32.565871Z",
          "shell.execute_reply": "2025-01-23T19:47:32.565188Z",
          "shell.execute_reply.started": "2025-01-23T19:47:32.406910Z"
        },
        "id": "tGk_y1w3Npwy",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "relevant = json.load(open('/kaggle/input/2wikimultihop/retrieved_contexts.json'))\n",
        "hard_negatives = json.load(open('/kaggle/input/2wikimultihop/hard_negatives.json'))\n",
        "hard_positives = json.load(open('/kaggle/input/2wikimultihop/positives.json'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1tZdUuhNpw0"
      },
      "source": [
        "## 1.2 Feed the retrieval results to the LLM and evaluate the accuracy using Exact Match (and metrics like ROUGE and BLEU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPFgRRbfNpw1"
      },
      "source": [
        "### Few-Shot Prompts Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-23T19:51:10.262925Z",
          "iopub.status.busy": "2025-01-23T19:51:10.262592Z",
          "iopub.status.idle": "2025-01-23T19:51:10.282693Z",
          "shell.execute_reply": "2025-01-23T19:51:10.281856Z",
          "shell.execute_reply.started": "2025-01-23T19:51:10.262897Z"
        },
        "id": "yXrRL3rMNpw1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
        "from sklearn.metrics import f1_score\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "import torch\n",
        "\n",
        "# Load JSON files\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# Map document IDs to their text\n",
        "def get_document_texts(doc_ids, document_data):\n",
        "    return [document_data[str(doc_id)][\"text\"] for doc_id in doc_ids if str(doc_id) in document_data]\n",
        "\n",
        "def get_mix(topk, negatives, ratio, N, query_id):\n",
        "    return topk[:N]\n",
        "\n",
        "# Retrieve top-k contexts\n",
        "def retrieve_contexts(query_id, contriever_data, hard_negs, document_data, ratio, N):\n",
        "    doc_ids = list(contriever_data.get(query_id, {}).keys())\n",
        "    hard_negs_id = list(hard_negs.get(query_id, {}).keys())\n",
        "    doc_ids = get_mix(doc_ids, hard_negs_id, ratio, N, query_id)\n",
        "    return get_document_texts(doc_ids, document_data)\n",
        "\n",
        "\n",
        "# Prepare input for LLM\n",
        "def prepare_input(query, contexts, zero_shot=False):\n",
        "    if zero_shot:\n",
        "        user_prompt = f\"Given the evidence, Evidence: {' '.join(contexts)} \\n use the information, think step by step and output the final answer extremely concisely in the form [Final Answer]: for the question, Question:{query}\"\n",
        "        return user_prompt\n",
        "    user_prompt = f\"\"\"[Question]: When does monsoon season end in the state the area code 575 is located?\n",
        "[Answer]: The area code 575 is located in New Mexico. Monsoon season in New Mexico typically ends in mid-September. So the\n",
        "[Final Answer]: mid-September.\n",
        "[Question]: What is the current official currency in the country where Ineabelle Diaz is a citizen?\n",
        "[Answer]: Ineabelle Diaz is from Peurto Rico, which is in the United States of America. The current official currency in the United\n",
        "States is the United States dollar.\n",
        "[Final Answer]: United States dollar.\n",
        "[Question]: Where was the person who founded the American Institute of Public Opinion in 1935 born?\n",
        "[Answer]: The person who founded the American Institute of Public Opinion in 1935 is George Gallup. George Gallup was born\n",
        "in Jefferson, Iowa.\n",
        "[Final Answer]: Jefferson.\n",
        "[Question]: What language is used by the director of Tiffany Memorandum?\n",
        "[Answer]: The director of Tiffany Memorandum is Sergio Grieco. Sergio Grieco speaks Italian.\n",
        "[Final Answer]: Italian.\n",
        "[Question]: What is the sports team the person played for who scored the first touchdown in Superbowl 1?\n",
        "[Answer]: The player that scored the first touchdown in Superbowl 1 is Max McGee. Max McGee played for the Green Bay\n",
        "Packers.\n",
        "[Final Answer]: Green Bay Packers.\n",
        "[Question]: The birth country of Jayantha Ketagoda left the British Empire when?\n",
        "[Answer]: The birth country of Jayantha Ketagoda is Sri Lanka. Sri Lanka left the British Empire on February 4, 1948. So the\n",
        "[Final Answer]: February 4, 1948.\\n\\n Follow the above example and Given the evidence, Evidence: {' '.join(contexts)} \\n use the information, think step by step and output the final answer extremely concisely in the form [Final Answer]: for the question, Question:{query}\"\"\"\n",
        "    return user_prompt\n",
        "\n",
        "# Generate response using LLM\n",
        "def generate_answer(input_text, tokenizer, model):\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True).to('cuda')\n",
        "    inputs.pop(\"token_type_ids\", None)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=1500)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# F1 Score Calculation\n",
        "def f1_score_metric(prediction, ground_truth):\n",
        "    pred_tokens = set(prediction.lower().split())\n",
        "    gt_tokens = set(ground_truth.lower().split())\n",
        "\n",
        "    intersection = pred_tokens.intersection(gt_tokens)\n",
        "    precision = len(intersection) / len(pred_tokens) if len(pred_tokens) > 0 else 0\n",
        "    recall = len(intersection) / len(gt_tokens) if len(gt_tokens) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    return f1, precision, recall\n",
        "\n",
        "# BLEU Score Calculation\n",
        "def bleu_score_metric(prediction, ground_truth):\n",
        "    prediction_tokens = prediction.lower().split()\n",
        "    ground_truth_tokens = ground_truth.lower().split()\n",
        "    score = sentence_bleu([ground_truth_tokens], prediction_tokens, smoothing_function=SmoothingFunction().method0)\n",
        "\n",
        "    return score\n",
        "\n",
        "# ROUGE Score Calculation\n",
        "def rouge_score_metric(prediction, ground_truth):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    scores = scorer.score(ground_truth, prediction)\n",
        "    return scores\n",
        "\n",
        "# Process dataset and evaluate responses\n",
        "def process_and_evaluate(contriever_data, dataset, hard_negs, document_data, tokenizer, model, ratio, N, zero_shot=False):\n",
        "    results = {}\n",
        "    question_df = {\"questions\":[],\"answers\":[]}\n",
        "    total_em = 0\n",
        "    total_f1 = 0\n",
        "    total_precision = 0\n",
        "    total_recall = 0\n",
        "    total_bleu = 0\n",
        "    total_rouge = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}\n",
        "    count = 0\n",
        "    failed = 0\n",
        "    print(f\"Processing with ratio {ratio}, N={N}\")\n",
        "    for entry in dataset:\n",
        "        if count % 100 == 0:\n",
        "          print(f\"Processed {100*count/1200}% of queries\")\n",
        "        if count == 1200:\n",
        "            break\n",
        "        else:\n",
        "            count += 1\n",
        "        query_id = entry[\"_id\"]\n",
        "        query = entry[\"question\"]\n",
        "        ground_truth = entry[\"answer\"]\n",
        "        # Retrieve contexts and prepare input\n",
        "        contexts = retrieve_contexts(query_id, contriever_data, hard_negs, document_data, ratio, N)\n",
        "        # print(contexts)\n",
        "        input_text = prepare_input(query, contexts, zero_shot)\n",
        "        # print(input_text)\n",
        "\n",
        "        # Generate response\n",
        "        try:\n",
        "            prediction = generate_answer(input_text, tokenizer, model)\n",
        "        except RuntimeError as e:\n",
        "            if failed == 0:\n",
        "                print(\"Fails start at:\", count)\n",
        "            failed += 1\n",
        "            continue\n",
        "\n",
        "\n",
        "        # Calculate Exact Match score\n",
        "        if \"not possible\" in prediction.lower() or \"unknown\" in prediction.lower():\n",
        "            em = 0\n",
        "        elif len(re.split(r'\\[?Final Answer\\]?:', prediction)) > 1:\n",
        "            answer = re.split(r'\\[?Final Answer\\]?:', prediction)[-1]\n",
        "            em = 1 if ground_truth.lower() in prediction.lower() else 0\n",
        "        else:\n",
        "            em = 0\n",
        "\n",
        "        total_em += em\n",
        "\n",
        "        # Calculate F1 Score\n",
        "        f1,precision, recall = f1_score_metric(re.split(r'\\[?Final Answer\\]?:', prediction)[-1], ground_truth)\n",
        "        total_f1 += f1\n",
        "        total_precision += precision\n",
        "        total_recall += recall\n",
        "\n",
        "        # Calculate BLEU Score\n",
        "        bleu = bleu_score_metric(re.split(r'\\[?Final Answer\\]?:', prediction)[-1], ground_truth)\n",
        "        total_bleu += bleu\n",
        "\n",
        "        # Calculate ROUGE Score\n",
        "        rouge = rouge_score_metric(re.split(r'\\[?Final Answer\\]?:', prediction)[-1], ground_truth)\n",
        "        total_rouge['rouge1'] += rouge['rouge1'].fmeasure\n",
        "        total_rouge['rouge2'] += rouge['rouge2'].fmeasure\n",
        "        total_rouge['rougeL'] += rouge['rougeL'].fmeasure\n",
        "\n",
        "        # Store results\n",
        "        results[query_id] = {\"prediction\": prediction, \"ground_truth\": ground_truth, \"exact_match\": em}\n",
        "\n",
        "        if len(re.split(r'\\[?Final Answer\\]?:', prediction)) > 1:\n",
        "            question_df[\"answers\"].append(re.split(r'\\[?Final Answer\\]?:', prediction)[-1])\n",
        "        else:\n",
        "            question_df[\"answers\"].append(prediction)\n",
        "\n",
        "        question_df[\"questions\"].append(query)\n",
        "\n",
        "    # Calculate overall metrics\n",
        "    accuracy = total_em / 1200\n",
        "    average_f1 = total_f1 / 1200\n",
        "    average_precision = total_precision / 1200\n",
        "    average_recall = total_recall / 1200\n",
        "    average_bleu = total_bleu / 1200\n",
        "    average_rouge1 = total_rouge['rouge1'] / 1200\n",
        "    average_rouge2 = total_rouge['rouge2'] / 1200\n",
        "    average_rougeL = total_rouge['rougeL'] / 1200\n",
        "\n",
        "    final_questions = pd.DataFrame(question_df)\n",
        "\n",
        "    print(\"N Failed:\", failed)\n",
        "    print(f\"Exact Match Accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(f\"Average F1 Score: {average_f1:.2f}\")\n",
        "    print(f\"Average Precision: {average_precision:.2f}\")\n",
        "    print(f\"Average Recall: {average_recall:.2f}\")\n",
        "    print(f\"Average BLEU Score: {average_bleu:.2f}\")\n",
        "    print(f\"Average ROUGE-1 F-Score: {average_rouge1:.2f}\")\n",
        "    print(f\"Average ROUGE-2 F-Score: {average_rouge2:.2f}\")\n",
        "    print(f\"Average ROUGE-L F-Score: {average_rougeL:.2f}\")\n",
        "\n",
        "    final_questions.to_csv(\"llama2_wqa_rag_5_few_shot_metrics.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "    return results, accuracy, average_f1, average_precision, average_recall, average_bleu, average_rouge1, average_rouge2, average_rougeL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "ef5046100850482ea98938c810db8645"
          ]
        },
        "execution": {
          "execution_failed": "2025-01-23T19:52:07.909Z",
          "iopub.execute_input": "2025-01-23T19:51:15.951339Z",
          "iopub.status.busy": "2025-01-23T19:51:15.950902Z"
        },
        "id": "h2tosAoINpw3",
        "outputId": "006e26d9-972d-4b86-e889-10de6bc5c229",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef5046100850482ea98938c810db8645",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "auth_token = os.getenv(\"huggingface_token\")\n",
        "\n",
        "if not auth_token:\n",
        "    raise ValueError(\"Authentication token not found. Please set HF_TOKEN in your .env file.\")\n",
        "\n",
        "login(auth_token)\n",
        "\n",
        "# File paths\n",
        "dataset_file_path = \"/kaggle/input/2wikimultihop/dev.json\"  # Replace with actual path\n",
        "document_file_path = \"/kaggle/input/2wikimultihop/wiki_musique_corpus.json\"  # Replace with actual path\n",
        "\n",
        "# Load data\n",
        "contriever_data = relevant\n",
        "dataset = load_json(dataset_file_path)\n",
        "document_data = load_json(document_file_path)\n",
        "\n",
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# Initialize LLM and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
        "\n",
        "ratios = [1]\n",
        "N = 5\n",
        "ZERO_SHOT = False # Change this if you want to do zero shot/few shot\n",
        "\n",
        "for ratio in ratios:\n",
        "    # Process dataset and evaluate\n",
        "    results, accuracy, average_f1, average_precision, average_recall, average_bleu, average_rouge1, average_rouge2, average_rougeL = process_and_evaluate(\n",
        "        contriever_data, dataset, hard_negatives, document_data, tokenizer, model, ratio, N, zero_shot=ZERO_SHOT)\n",
        "\n",
        "    print(f\"Results for ratio {ratio}, N={N}\")\n",
        "    # Print evaluation results\n",
        "    print(f\"Overall Exact Match Accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(f\"Average F1 Score: {average_f1:.2f}\")\n",
        "    print(f\"Average Precision: {average_precision:.2f}\")\n",
        "    print(f\"Average Recall: {average_recall:.2f}\")\n",
        "    print(f\"Average BLEU Score: {average_bleu:.2f}\")\n",
        "    print(f\"Average ROUGE-1 F-Score: {average_rouge1:.2f}\")\n",
        "    print(f\"Average ROUGE-2 F-Score: {average_rouge2:.2f}\")\n",
        "    print(f\"Average ROUGE-L F-Score: {average_rougeL:.2f}\")\n",
        "\n",
        "    with open(f\"/kaggle/working/accuracy_metrics_{ratio}_{N}_{ZERO_SHOT}.txt\", \"w\") as f:\n",
        "        f.write(f\"Overall Exact Match Accuracy: {accuracy * 100:.2f}%\\n\")\n",
        "        f.write(f\"Average F1 Score: {average_f1:.2f}\\n\")\n",
        "        f.write(f\"Average Precision: {average_precision:.2f}\\n\")\n",
        "        f.write(f\"Average Recall: {average_recall:.2f}\\n\")\n",
        "        f.write(f\"Average BLEU Score: {average_bleu:.2f}\\n\")\n",
        "        f.write(f\"Average ROUGE-1 F-Score: {average_rouge1:.2f}\\n\")\n",
        "        f.write(f\"Average ROUGE-2 F-Score: {average_rouge2:.2f}\\n\")\n",
        "        f.write(f\"Average ROUGE-L F-Score: {average_rougeL:.2f}\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 6336369,
          "sourceId": 10245429,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6336405,
          "sourceId": 10245476,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6336354,
          "sourceId": 10245499,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6401353,
          "sourceId": 10337803,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6481969,
          "sourceId": 10480095,
          "sourceType": "datasetVersion"
        }
      ],
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
